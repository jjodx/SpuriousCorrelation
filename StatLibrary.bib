
@article{lakens2017,
  title = {Too True to Be Bad},
  issn = {1948-5506},
  abstract = {Psychology journals rarely publish non-significant results. At the same time, it is often very unlikely (or `too good to be true') that a set of studies yields exclusively significant results. Here, we use likelihood ratios to explain when sets of studies that contain a mix of significant and non-significant results are likely to be true, or `too true to be bad'. As we show, mixed results are not only likely to be observed in lines of research, but when observed, mixed results often provide evidence for the alternative hypothesis, given reasonable levels of statistical power and an adequately controlled low Type 1 error rate. Researchers should feel comfortable submitting such lines of research with an internal meta-analysis for publication. A better understanding of probabilities, accompanied by more realistic expectations of what real lines of studies look like, might be an important step in mitigating publication bias in the scientific literature.},
  journal = {Social Psychological and Personality Science},
  doi = {10.1177/1948550617693058},
  author = {Lakens, Dani{\"e}l and Etz, A. J.},
  year = {2017},
  keywords = {likelihoods,power,publication bias,statistical inferences},
  pages = {1--22}
}

@article{lakens2014a,
  title = {Performing High-Powered Studies Efficiently with Sequential Analyses},
  volume = {44},
  issn = {00462772},
  number = {7},
  journal = {European Journal of Social Psychology},
  doi = {10.1002/ejsp.2023},
  author = {Lakens, Dani{\"e}l},
  month = dec,
  year = {2014},
  pages = {701--710}
}

@article{lakens2013,
  title = {Calculating and Reporting Effect Sizes to Facilitate Cumulative Science: A Practical Primer for t-Tests and {{ANOVAs}}.},
  volume = {4},
  issn = {1664-1078},
  abstract = {Effect sizes are the most important outcome of empirical studies. Most articles on effect sizes highlight their importance to communicate the practical significance of results. For scientists themselves, effect sizes are most useful because they facilitate cumulative science. Effect sizes can be used to determine the sample size for follow-up studies, or examining effects across studies. This article aims to provide a practical primer on how to calculate and report effect sizes for t-tests and ANOVA's such that effect sizes can be used in a-priori power analyses and meta-analyses. Whereas many articles about effect sizes focus on between-subjects designs and address within-subjects designs only briefly, I provide a detailed overview of the similarities and differences between within- and between-subjects designs. I suggest that some research questions in experimental psychology examine inherently intra-individual effects, which makes effect sizes that incorporate the correlation between measures the best summary of the results. Finally, a supplementary spreadsheet is provided to make it as easy as possible for researchers to incorporate effect size calculations into their workflow.},
  number = {November},
  journal = {Frontiers in psychology},
  doi = {10.3389/fpsyg.2013.00863},
  author = {Lakens, Dani{\"e}l},
  month = jan,
  year = {2013},
  keywords = {power analysis,cohen,cohen's d,effect greater than zero,effect sizes,effect sizes are the,empirical studies,eta-squar,eta-squared,experi-,mental manipulation has an,most important outcome of,or,researchers want to know,s d,sample size planning,when,whether an intervention or},
  pages = {863},
  pmid = {24324449}
}

@article{lakens2014,
  title = {Performing High-Powered Studies Efficiently with Sequential Analyses},
  volume = {44},
  issn = {00462772},
  number = {7},
  journal = {European Journal of Social Psychology},
  doi = {10.1002/ejsp.2023},
  author = {Lakens, Dani{\"e}l},
  month = dec,
  year = {2014},
  pages = {701--710}
}

@article{gagnier2017a,
  title = {Misconceptions, {{Misuses}}, and {{Misinterpretations}} of {{P Values}} and {{Significance Testing}}},
  volume = {99},
  issn = {0021-9355},
  number = {18},
  journal = {The Journal of Bone and Joint Surgery},
  doi = {10.2106/JBJS.16.01314},
  author = {Gagnier, Joel J. and Morgenstern, Hal},
  month = sep,
  year = {2017},
  pages = {1598--1603}
}

@article{albers2018,
  title = {When Power Analyses Based on Pilot Data Are Biased: {{Inaccurate}} Effect Size Estimators and Follow-up Bias},
  volume = {74},
  issn = {10960465},
  abstract = {When designing a study, the planned sample size is often based on power analyses. One way to choose an effect size for power analyses is by relying on pilot data. A-priori power analyses are only accurate when the effect size estimate is accurate. In this paper we highlight two sources of bias when performing a-priori power analyses for between-subject designs based on pilot data. First, we examine how the choice of the effect size index ({$\eta$}2, \${\o}mega\$2and {$\epsilon$}2) affects the sample size and power of the main study. Based on our observations, we recommend against the use of {$\eta$}2in a-priori power analyses. Second, we examine how the maximum sample size researchers are willing to collect in a main study (e.g. due to time or financial constraints) leads to overestimated effect size estimates in the studies that are performed. Determining the required sample size exclusively based on the effect size estimates from pilot data, and following up on pilot studies only when the sample size estimate for the main study is considered feasible, creates what we term follow-up bias. We explain how follow-up bias leads to underpowered main studies. Our simulations show that designing main studies based on effect sizes estimated from small pilot studies does not yield desired levels of power due to accuracy bias and follow-up bias, even when publication bias is not an issue. We urge researchers to consider alternative approaches to determining the sample size of their studies, and discuss several options.},
  number = {April 2016},
  journal = {Journal of Experimental Social Psychology},
  doi = {10.1016/j.jesp.2017.09.004},
  author = {Albers, Casper and Lakens, Dani{\"e}l},
  year = {2018},
  keywords = {Effect size,Epsilon-squared,Eta-squared,Follow-up bias,Omega-squared,Power analysis},
  pages = {187--195}
}

@article{schimmack2012,
  title = {The {{Ironic Effect}} of {{Significant Results}} on the {{Credibility}} of {{Multiple}}-{{Study Articles}}.},
  volume = {17},
  issn = {1082-989X},
  abstract = {Cohen (1962) pointed out the importance of statistical power for psychology as a science, but statistical power of studies has not increased, while the number of studies in a single article has increased. It has been overlooked that multiple studies with modest power have a high probability of producing nonsignificant results because power decreases as a function of the number of statistical tests that are being conducted (Maxwell, 2004). The discrepancy between the expected number of significant results and the actual number of significant results in multiple-study articles undermines the credibility of the reported results, and it is likely that questionable research practices have contributed to the reporting of too many significant results (Sterling, 1959). The problem of low power in multiple-study articles is illustrated using Bem's (2011) article on extrasensory perception and Gailliot et al.'s (2007) article on glucose and self-regulation. I conclude with several recommendations that can increase the credibility of scientific evidence in psychological journals. One major recommendation is to pay more attention to the power of studies to produce positive results without the help of questionable research practices and to request that authors justify sample sizes with a priori predictions of effect sizes. It is also important to publish replication studies with nonsignificant results if these studies have high power to replicate a published finding.},
  number = {4},
  journal = {Psychological Methods},
  doi = {10.1037/a0029487},
  author = {Schimmack, Ulrich},
  month = dec,
  year = {2012},
  keywords = {p,power,publication bias,cohen,sample size,1304,1990,credibility,except of course for,in 2011,less is more,personality and social,psychology published an article,significance,that provided empirical support,the prestigious journal of},
  pages = {551--566},
  pmid = {22924598}
}

@article{lakens2018,
  title = {Justify Your Alpha},
  volume = {2},
  copyright = {All rights reserved},
  issn = {23973374},
  abstract = {In response to recommendations to redefine statistical significance to P {$\leq$} 0.005, we propose that researchers should transparently report and justify all choices they make when designing a study, including the alpha level.},
  number = {3},
  journal = {Nature Human Behaviour},
  doi = {10.1038/s41562-018-0311-x},
  author = {Lakens, Daniel and Adolfi, Federico G. and Albers, Casper J. and Anvari, Farid and Apps, Matthew A.J. and Argamon, Shlomo E. and Baguley, Thom and Becker, Raymond B. and Benning, Stephen D. and Bradford, Daniel E. and Buchanan, Erin M. and Caldwell, Aaron R. and Van Calster, Ben and Carlsson, Rickard and Chen, Sau Chin and Chung, Bryan and Colling, Lincoln J. and Collins, Gary S. and Crook, Zander and Cross, Emily S. and Daniels, Sameera and Danielsson, Henrik and Debruine, Lisa and Dunleavy, Daniel J. and Earp, Brian D. and Feist, Michele I. and Ferrell, Jason D. and Field, James G. and Fox, Nicholas W. and Friesen, Amanda and Gomes, Caio and {Gonzalez-Marquez}, Monica and Grange, James A. and Grieve, Andrew P. and Guggenberger, Robert and Grist, James and Van Harmelen, Anne Laura and Hasselman, Fred and Hochard, Kevin D. and Hoffarth, Mark R. and Holmes, Nicholas P. and Ingre, Michael and Isager, Peder M. and Isotalus, Hanna K. and Johansson, Christer and Juszczyk, Konrad and Kenny, David A. and Khalil, Ahmed A. and Konat, Barbara and Lao, Junpeng and Larsen, Erik Gahner and Lodder, Gerine M.A. and Lukavsk{\'y}, Ji{\v r}{\'i} and Madan, Christopher R. and Manheim, David and Martin, Stephen R. and Martin, Andrea E. and Mayo, Deborah G. and McCarthy, Randy J. and McConway, Kevin and McFarland, Colin and Nio, Amanda Q.X. and Nilsonne, Gustav and De Oliveira, Cilene Lino and {Orban de Xivry}, Jean-Jacques and Parsons, Sam and Pfuhl, Gerit and Quinn, Kimberly A. and Sakon, John J. and Saribay, S. Adil and Schneider, Iris K. and Selvaraju, Manojkumar and Sjoerds, Zsuzsika and Smith, Samuel G. and Smits, Tim and Spies, Jeffrey R. and Sreekumar, Vishnu and Steltenpohl, Crystal N. and Stenhouse, Neil and {{\'S}wi\textbackslash{}c atkowski}, Wojciech and Vadillo, Miguel A. and Van Assen, Marcel A.L.M. and Williams, Matt N. and Williams, Samantha E. and Williams, Donald R. and Yarkoni, Tal and Ziano, Ignazio and Zwaan, Rolf A.},
  year = {2018},
  pages = {168--171}
}

@article{pernet2013,
  title = {Robust Correlation Analyses: {{False}} Positive and Power Validation Using a New Open Source Matlab Toolbox},
  volume = {3},
  issn = {16641078},
  abstract = {Pearson's correlation measures the strength of the association between two variables. The technique is, however, restricted to linear associations and is overly sensitive to outliers. Indeed, a single outlier can result in a highly inaccurate summary of the data. Yet, it remains the most commonly used measure of association in psychology research. Here we describe a free Matlab((R)) based toolbox (http://sourceforge.net/projects/robustcorrtool/) that computes robust measures of association between two or more random variables: the percentage-bend correlation and skipped-correlations. After illustrating how to use the toolbox, we show that robust methods, where outliers are down weighted or removed and accounted for in significance testing, provide better estimates of the true association with accurate false positive control and without loss of power. The different correlation methods were tested with normal data and normal data contaminated with marginal or bivariate outliers. We report estimates of effect size, false positive rate and power, and advise on which technique to use depending on the data at hand.},
  number = {JAN},
  journal = {Frontiers in Psychology},
  doi = {10.3389/fpsyg.2012.00606},
  author = {Pernet, Cyril R. and Wilcox, Rand and Rousselet, Guillaume A.},
  year = {2013},
  keywords = {Correlation,Matlab,Outliers,Power,Robust statistics},
  pages = {1--18},
  pmid = {23335907}
}

@article{lane2016,
  title = {Is There a Publication Bias in Behavioral Intranasal Oxytocin Research on Humans? {{Opening}} the File Drawer of One Lab},
  issn = {09538194},
  abstract = {The neurohormone oxytocin (OT) has been one the most studied peptides in behavioral sciences over the past two decades. Primarily known for its crucial role in labor and lactation, a rapidly growing literature suggests that intranasal OT (IN-OT) may also play a role in humans' emotional and social lives. However, the lack of a convincing theoretical framework explaining IN- OT's effects that would also allow to predict which moderators exert their effects and when, has raised healthy skepticism regarding the robustness of human behavioral IN-OT research. The poor knowledge of OT's exact pharmacokinetic properties, crucial statistical and methodological issues and the absence of direct replication efforts may have lead to a publication bias in IN-OT literature with many unpublished studies with null results lying in laboratories' drawers. Is there a file drawer problem in IN-OT research? If this is the case, it may also be the case in our laboratory. This paper aims to answer that question, document the extent of the problem and discuss its implications for OT research. Through eight studies (including 13 dependent variables overall, assessed through 25 different paradigms) performed in our lab between 2009 and 2014 on 453 subjects, results were too often not those expected. Only five publications emerged from our studies and only one of these reported a null-finding. After realizing that our publication portfolio has become less and less representative of our actual findings and because the non-publication of our data might contribute to generating a publication bias in IN-OT research, we decided to get these studies out of our drawer and encourage other laboratories to do the same.},
  number = {16},
  journal = {Journal of Neuroendocrinology},
  doi = {10.1111/jne.12384},
  author = {Lane, Anthony and Luminet, Olivier and Nave, Gideon and Mikolajczak, Mo{\"i}ra},
  year = {2016},
  keywords = {10,doi,1111,12384,behavioural scientists have been,effects of the neuropeptide,file drawer,in humans for over,intranasal oxytocin,investigating the psychosocial,jne,laboratory report,ot,oxytocin},
  pages = {n/a--n/a},
  pmid = {26991328}
}

@article{colquhoun2014,
  title = {An Investigation of the False Discovery Rate and the Misinterpretation of {{P}} Values},
  issn = {2054-5703},
  abstract = {If you use P = 0.05 to suggest that you have made a discovery, you'll be wrong at least 30\% of the time. If, as is often the case, experiments are under-powered, you'll de wrong most of the time. This conclusion is demonstrated from several points of view. First, tree diagrams which show the close analogy with the screening test problem. Similar conclusions are drawn by repeated simulations of t tests. These mimic what's done in real life, which makes the results more persuasive The simulation method is used is used also to evaluate the extent effect sizes are over-estimated, especially in under-powered experiments. A script is supplied to allow the reader to do simulations themselves, with numbers appropriate for their own work. The interpretation of an observed result of, say, P = 0.047 is investigated from several points of view. It is concluded that if you wish to keep your false discovery rate below 5\%, you need to use a 3-sigma rule, or to insist on P {$\leq$} 0.001. And never use the word ``significant''.},
  journal = {Hazards of P values},
  doi = {10.1098/rsos.140216},
  author = {Colquhoun, David and London, College},
  year = {2014},
  keywords = {computational biology,statistics},
  pages = {1--15}
}

@article{lakens2014b,
  title = {Sailing {{From}} the {{Seas}} of {{Chaos Into}} the {{Corridor}} of {{Stability}}: {{Practical Recommendations}} to {{Increase}} the {{Informational Value}} of {{Studies}}},
  volume = {9},
  issn = {1745-6916},
  number = {3},
  journal = {Perspectives on Psychological Science},
  doi = {10.1177/1745691614528520},
  author = {Lakens, Dani{\"e}l and Evers, E. R. K.},
  month = may,
  year = {2014},
  keywords = {confidence intervals,induction,p -curve analysis,sequential analyses,v statistic},
  pages = {278--292}
}

@article{francis2014,
  title = {The Frequency of Excess Success for Articles in {{Psychological Science}}},
  journal = {Psychonomic bulletin \& review},
  author = {Francis, Gregory},
  year = {2014},
  pages = {1--26}
}

@article{lakens2013a,
  title = {On the {{Benefits}} of {{Adaptive Designs}} and {{Sequential Analyses}} for {{Psychological Science}}},
  journal = {Available at SSRN 2333729},
  author = {Lakens, Dani{\"e}l},
  year = {2013},
  pages = {1--22}
}

@article{bennett2011,
  title = {Neural Correlates of Interspecies Perspective Taking in the Post-Mortem Atlantic Salmon: An Argument for Proper Multiple Comparisons Correction},
  volume = {1},
  number = {1},
  journal = {Journal of Serendipitous \textbackslash{}ldots},
  author = {Bennett, CM and Baird, AA and Miller, Michael B and Wolford, George L},
  year = {2011},
  keywords = {edu,bennett,contact,fmri statistics fdr fwer,psych,ucsb},
  pages = {1--5}
}

@article{orbandexivry2009a,
  title = {Smooth Pursuit Performance during Target Blanking Does Not Influence the Triggering of Predictive Saccades.},
  volume = {9},
  copyright = {All rights reserved},
  issn = {1534-7362},
  abstract = {Visually guided catch-up saccades during the pursuit of a moving target are highly influenced by smooth pursuit performance. For example, the decision to execute a saccade and its amplitude is driven by the difference in velocity between the eye and the target. In previous studies, we have demonstrated that the predictive saccades that occur during the blanking of the moving target compensate for the variability of the smooth pursuit response. Therefore, we wondered whether the predictive smooth pursuit response during target blanking influenced the occurrence of predictive saccades, which is the case for visually guided catch-up saccades. To answer this question, we asked subjects to track visually a target moving along a circular path. From time to time, the target was unexpectedly blanked for some randomized durations and disappeared from the screen. Surprisingly, we did not find any differences in smooth pursuit performance between the blanks that did and those that did not contain predictive saccades. In addition, during the blanks, the differences in smooth pursuit performance across the sessions or across the subjects did not correlate with the differences in the number of predictive saccades. Therefore, this study demonstrates that smooth pursuit performance does not influence the occurrence of predictive saccades. We interpret these results in light of the possible minimization of position error at target reappearance, which heavily depends on the saccadic amplitudes but not on their timing.},
  number = {11},
  journal = {Journal of vision},
  doi = {10.1167/9.11.7},
  author = {{Orban de Xivry}, Jean-Jacques and Missal, Marcus and Lef{\`e}vre, Philippe},
  month = jan,
  year = {2009},
  keywords = {Adult,Humans,Motion Perception,Motion Perception: physiology,Photic Stimulation,Photic Stimulation: methods,Predictive Value of Tests,Pursuit,Reaction Time,Reaction Time: physiology,Saccades,Saccades: physiology,Smooth,Smooth: physiology,Young Adult},
  pages = {7.1--16},
  pmid = {20053070}
}

@article{joober2012,
  title = {Publication Bias: {{What}} Are the Challenges and Can They Be Overcome?},
  volume = {37},
  issn = {1180-4882},
  shorttitle = {Publication Bias},
  number = {3},
  journal = {Journal of Psychiatry \& Neuroscience : JPN},
  doi = {10.1503/jpn.120065},
  author = {Joober, Ridha and Schmitz, Norbert and Annable, Lawrence and Boksa, Patricia},
  month = may,
  year = {2012},
  pages = {149-152},
  pmid = {22515987},
  pmcid = {PMC3341407}
}

@techreport{bryan2017,
  title = {Excuse Me, Do You Have a Moment to Talk about Version Control?},
  abstract = {Data analysis, statistical research, and teaching statistics have at least one thing in common: these activities all produce many files! There are data files, source code, figures, tables, prepared reports, and much more. Most of these files evolve over the course of a project and often need to be shared with others, for reading or edits, as a project unfolds. Without explicit and structured management, project organization can easily descend into chaos, taking time away from the primary work and reducing the quality of the final product. This unhappy result can be avoided by repurposing tools and workflows from the software development world, namely, distributed version control. This article describes the use of the version control system Git and and the hosting site GitHub for statistical and data scientific workflows. Special attention is given to projects that use the statistical language R and, optionally, R Markdown documents. Supplementary materials include an annotated set of links to step-by-step tutorials, real world examples, and other useful learning resources.},
  language = {en},
  number = {e3159v2},
  institution = {{PeerJ Inc.}},
  author = {Bryan, Jennifer},
  month = aug,
  year = {2017},
  file = {C:\\Users\\u0099946\\Zotero\\storage\\XX8ENTBA\\3159.html},
  doi = {10.7287/peerj.preprints.3159v2}
}

@article{howard2016,
  title = {A {{Review}} of {{Exploratory Factor Analysis Decisions}} and {{Overview}} of {{Current Practices}}: {{What We Are Doing}} and {{How Can We Improve}}?},
  volume = {32},
  issn = {1044-7318},
  shorttitle = {A {{Review}} of {{Exploratory Factor Analysis Decisions}} and {{Overview}} of {{Current Practices}}},
  abstract = {Authors within the fields of cyberpsychology and human-computer interaction have demonstrated a particular interest in measurement and scale creation, and exploratory factor analysis (EFA) is an extremely important statistical method for these areas of research. Unfortunately, EFA requires several statistical and methodological decisions to which the best choices are often unclear. The current article reviews five primary decisions and provides direct suggestions for best practices. These decisions are (a) the data inspection techniques, (b) the factor analytic method, (c) the factor retention method, (d) the factor rotation method, and (e) the factor loading cutoff. Then the article reviews authors' choices for these five EFA decisions in every relevant article within seven cyberpsychology and/or human\textendash{}computer interaction journals. The results demonstrate that authors do not employ the recommended best practices for most decisions. Particularly, most authors do not inspect their data for violations of assumptions, apply inappropriate factor analytic methods, utilize outdated factor retention methods, and omit the justification for their factor rotation methods. Further, many authors omit altogether their EFA decisions. To rectify these concerns, the current article provides a step-by-step guide and checklist that authors can reference to ensure the use of recommended best practices. Together, the current article identifies concerns with current research and provides direct solutions to these concerns.},
  number = {1},
  journal = {International Journal of Human\textendash{}Computer Interaction},
  doi = {10.1080/10447318.2015.1087664},
  author = {Howard, Matt C.},
  month = jan,
  year = {2016},
  pages = {51-62}
}

@article{greenland2016,
  title = {Statistical Tests, {{P}} Values, Confidence Intervals, and Power: A Guide to Misinterpretations},
  volume = {31},
  issn = {1573-7284},
  shorttitle = {Statistical Tests, {{P}} Values, Confidence Intervals, and Power},
  abstract = {Misinterpretation and abuse of statistical tests, confidence intervals, and statistical power have been decried for decades, yet remain rampant. A key problem is that there are no interpretations of these concepts that are at once simple, intuitive, correct, and foolproof. Instead, correct use and interpretation of these statistics requires an attention to detail which seems to tax the patience of working scientists. This high cognitive demand has led to an epidemic of shortcut definitions and interpretations that are simply wrong, sometimes disastrously so\textemdash{}and yet these misinterpretations dominate much of the scientific literature. In light of this problem, we provide definitions and a discussion of basic statistics that are more general and critical than typically found in traditional introductory expositions. Our goal is to provide a resource for instructors, researchers, and consumers of statistics whose knowledge of statistical theory and technique may be limited but who wish to avoid and spot misinterpretations. We emphasize how violation of often unstated analysis protocols (such as selecting analyses for presentation based on the P values they produce) can lead to small P values even if the declared test hypothesis is correct, and can lead to large P values even if that hypothesis is incorrect. We then provide an explanatory list of 25 misinterpretations of P values, confidence intervals, and power. We conclude with guidelines for improving statistical interpretation and reporting.},
  language = {en},
  number = {4},
  journal = {European Journal of Epidemiology},
  doi = {10.1007/s10654-016-0149-3},
  author = {Greenland, Sander and Senn, Stephen J. and Rothman, Kenneth J. and Carlin, John B. and Poole, Charles and Goodman, Steven N. and Altman, Douglas G.},
  month = apr,
  year = {2016},
  keywords = {Hypothesis testing,Confidence intervals,Power,P value,Null testing,Significance tests,Statistical testing},
  pages = {337-350}
}

@misc{elson2017,
  title = {{{FlexibleMeasures}}.Com: {{Go}}/{{No}}-{{Go Task}} Doi:10.17605/{{OSF}}.{{IO}}/{{GSX52}}},
  shorttitle = {{{FlexibleMeasures}}.Com},
  abstract = {Archiving examples of flexible measures in psychology to encourage reflection about both the use of flexible measures in research and the validity of studies that rely on them},
  language = {English},
  journal = {Elson, M. (2017). FlexibleMeasures.com: Go/No-Go Task. doi:10.17605/OSF.IO/GSX52},
  howpublished = {http://www.flexiblemeasures.com/nogo/},
  author = {Elson, Malte},
  year = {2017}
}

@article{devito2019,
  title = {Catalogue of Bias: Publication Bias},
  volume = {24},
  copyright = {\textcopyright{} Author(s) (or their employer(s)) 2019. No commercial re-use. See rights and permissions. Published by BMJ.},
  issn = {2515-446X, 2515-4478},
  shorttitle = {Catalogue of Bias},
  abstract = {Dickersin and Min define publication bias as the failure to publish the results of a study `on the basis of the direction or strength of the study findings'.1 This non-publication introduces a bias which impacts the ability to accurately synthesise and describe the evidence in a given area.2 Publication bias is a type of reporting bias and closely related to dissemination bias, although dissemination bias generally applies to all forms of results dissemination, not simply journal publications. A variety of distinct biases are often grouped into the overall definition of publication bias.3 4 

There are a number of risk factors and causes for publication bias identified in the literature.5 Research has shown causes of publication bias ranging from trialist motivation, past experience, and competing commitments; perceived or real lack of interest in results from editors, reviewers,~or other colleagues; or conflicts of interest that would lead to the suppression of results not aligned with a specific agenda.3 6\textendash{}9 The role of journal editors is particularly complex as the gatekeepers to publication. Significant results are more widely cited in medicine aligning the incentives of both investigators and editors towards these studies.10 A review by Song and colleagues reports studies showing that strength and \ldots{}},
  language = {en},
  number = {2},
  journal = {BMJ Evidence-Based Medicine},
  doi = {10.1136/bmjebm-2018-111107},
  author = {DeVito, Nicholas J. and Goldacre, Ben},
  month = apr,
  year = {2019},
  keywords = {medical ethics},
  pages = {53-54},
  file = {C:\\Users\\u0099946\\Zotero\\storage\\697F6SFU\\53.html},
  pmid = {30523135}
}

@article{joober2012a,
  title = {Publication Bias: {{What}} Are the Challenges and Can They Be Overcome?},
  volume = {37},
  issn = {1180-4882},
  shorttitle = {Publication Bias},
  number = {3},
  journal = {Journal of Psychiatry \& Neuroscience : JPN},
  doi = {10.1503/jpn.120065},
  author = {Joober, Ridha and Schmitz, Norbert and Annable, Lawrence and Boksa, Patricia},
  month = may,
  year = {2012},
  pages = {149-152},
  pmid = {22515987},
  pmcid = {PMC3341407}
}

@article{mlinaric2017,
  title = {Dealing with the Positive Publication Bias: {{Why}} You Should Really Publish Your Negative Results},
  volume = {27},
  issn = {1330-0962},
  shorttitle = {Dealing with the Positive Publication Bias},
  abstract = {Studies with positive results are greatly more represented in literature than studies with negative results, producing so-called publication bias. This review aims to discuss occurring problems around negative results and to emphasize the importance of reporting negative results. Underreporting of negative results introduces bias into meta-analysis, which consequently misinforms researchers, doctors and policymakers. More resources are potentially wasted on already disputed research that remains unpublished and therefore unavailable to the scientific community. Ethical obligations need to be considered when reporting results of studies on human subjects as people have exposed themselves to risk with the assurance that the study is performed to benefit others. Some studies disprove the common conception that journal editors preferably publish positive findings, which are considered as more citable. Therefore, all stakeholders, but especially researchers, need to be conscious of disseminating negative and positive findings alike.},
  number = {3},
  journal = {Biochemia Medica},
  doi = {10.11613/BM.2017.030201},
  author = {Mlinari{\'c}, Ana and Horvat, Martina and {\v S}upak Smol{\v c}i{\'c}, Vesna},
  month = oct,
  year = {2017},
  pmid = {29180912},
  pmcid = {PMC5696751}
}

@article{bonett2000,
  title = {Sample Size Requirements for Estimating Pearson, Kendall and Spearman Correlations},
  volume = {65},
  issn = {1860-0980},
  abstract = {Interval estimates of the Pearson, Kendall tau-a and Spearman correlations are reviewed and an improved standard error for the Spearman correlation is proposed. The sample size required to yield a confidence interval having the desired width is examined. A two-stage approximation to the sample size requirement is shown to give accurate results.},
  language = {en},
  number = {1},
  journal = {Psychometrika},
  doi = {10.1007/BF02294183},
  author = {Bonett, Douglas G. and Wright, Thomas A.},
  month = mar,
  year = {2000},
  keywords = {sample size,correlation,interval estimation,rank correlation},
  pages = {23-28}
}

@article{havlicek1976,
  title = {Robustness of the {{Pearson Correlation}} against {{Violations}} of {{Assumptions}}},
  volume = {43},
  issn = {0031-5125},
  abstract = {The purpose of this study was to determine empirically effects of the violation of assumptions of normality and of measurement scales on the Pearson product-moment correlation coefficient. The effects of such violations were studied separately and in combination for samples of varying size from 5 to 60. Monte Carlo procedures were used to generate populations of scores for four basic distributions: normal, positively skewed, negatively skewed, and leptokurtic. Samples of varying sizes were then randomly selected from specific populations. Results of the study were based on distributions of rs which were calculated on 5,000 sets of samples of n = 5 or n = 15 and 3,000 sets of samples of n = 30 and n = 60. Results indicated that the Pearson r is insensitive to rather extreme violations of the basic assumptions of normality and type of measurement scale. Failure to meet the basic assumptions separately or in combinations had little effect upon the obtained distributions of rs.},
  language = {en},
  number = {3\_suppl},
  journal = {Perceptual and Motor Skills},
  doi = {10.2466/pms.1976.43.3f.1319},
  author = {Havlicek, Larry L. and Peterson, Nancy L.},
  month = dec,
  year = {1976},
  pages = {1319-1334}
}

@article{mukaka2012,
  title = {A Guide to Appropriate Use of {{Correlation}} Coefficient in Medical Research},
  volume = {24},
  copyright = {Copyright for articles published in this journal is retained by the journal.},
  issn = {1995-7262},
  abstract = {Correlation is a statistical method used to assess a possible linear association between two continuous variables. It is simple both to calculate and to interpret. However, misuse of correlation is so common among researchers that some statisticians have wished that the method had never been devised at all. The aim of this article is to provide a guide to appropriate use of correlation in medical research and to highlight some misuse. Examples of the applications of the correlation coefficient have been provided using data from statistical simulations as well as real data. Rule of thumb for interpreting size of a correlation coefficient has been provided},
  language = {en},
  number = {3},
  journal = {Malawi Medical Journal},
  author = {Mukaka, M. M.},
  month = jan,
  year = {2012},
  pages = {69-71-71},
  file = {C:\\Users\\u0099946\\Zotero\\storage\\QVWQP563\\81576.html}
}

@article{leerodgers1988,
  title = {Thirteen {{Ways}} to {{Look}} at the {{Correlation Coefficient}}},
  volume = {42},
  issn = {0003-1305},
  abstract = {In 1885, Sir Francis Galton first defined the term ``regression'' and completed the theory of bivariate correlation. A decade later, Karl Pearson developed the index that we still use to measure correlation, Pearson's r. Our article is written in recognition of the 100th anniversary of Galton's first discussion of regression and correlation. We begin with a brief history. Then we present 13 different formulas, each of which represents a different computational and conceptual definition of r. Each formula suggests a different way of thinking about this index, from algebraic, geometric, and trigonometric settings. We show that Pearson's r (or simple functions of r) may variously be thought of as a special type of mean, a special type of variance, the ratio of two means, the ratio of two variances, the slope of a line, the cosine of an angle, and the tangent to an ellipse, and may be looked at from several other interesting perspectives.},
  number = {1},
  journal = {The American Statistician},
  doi = {10.1080/00031305.1988.10475524},
  author = {Lee Rodgers, Joseph and Nicewander, W.   Alan},
  month = feb,
  year = {1988},
  pages = {59-66},
  file = {C:\\Users\\u0099946\\Zotero\\storage\\L5SIGJMW\\00031305.1988.html}
}

@article{schober2018,
  title = {Correlation {{Coefficients}}: {{Appropriate Use}} and {{Interpretation}}},
  volume = {126},
  issn = {0003-2999},
  shorttitle = {Correlation {{Coefficients}}},
  abstract = {Correlation in the broadest sense is a measure of an association between variables. In correlated data, the change in the magnitude of 1 variable is associated with a change in the magnitude of another variable, either in the same (positive correlation) or in the opposite (negative correlation) direction. Most often, the term correlation is used in the context of a linear relationship between 2 continuous variables and expressed as Pearson product-moment correlation. The Pearson correlation coefficient is typically used for jointly normally distributed data (data that follow a bivariate normal distribution). For nonnormally distributed continuous data, for ordinal data, or for data with relevant outliers, a Spearman rank correlation can be used as a measure of a monotonic association. Both correlation coefficients are scaled such that they range from \textendash{}1 to +1, where 0 indicates that there is no linear or monotonic association, and the relationship gets stronger and ultimately approaches a straight line (Pearson correlation) or a constantly increasing or decreasing curve (Spearman correlation) as the coefficient approaches an absolute value of 1. Hypothesis tests and confidence intervals can be used to address the statistical significance of the results and to estimate the strength of the relationship in the population from which the data were sampled. The aim of this tutorial is to guide researchers and clinicians in the appropriate use and interpretation of correlation coefficients.},
  language = {en-US},
  number = {5},
  journal = {Anesthesia \& Analgesia},
  doi = {10.1213/ANE.0000000000002864},
  author = {Schober, Patrick and Boer, Christa and Schwarte, Lothar A.},
  month = may,
  year = {2018},
  pages = {1763},
  file = {C:\\Users\\u0099946\\Zotero\\storage\\CTIRYUWP\\Correlation_Coefficients__Appropriate_Use_and.50.html}
}

@article{taylor1990,
  title = {Interpretation of the {{Correlation Coefficient}}: {{A Basic Review}}},
  volume = {6},
  issn = {8756-4793},
  shorttitle = {Interpretation of the {{Correlation Coefficient}}},
  abstract = {A basic consideration in the evaluation of professional medical literature is being able to understand the statistical analysis presented. One of the more frequently reported statistical methods involves correlation analysis where a correlation coefficient is reported representing the degree of linear association between two variables. This article discusses the basic aspects of correlation analysis with examples given from professional journals and focuses on the interpretations and limitations of the correlation coefficient. No attention was given to the actual calculation of this statistical value.},
  language = {en},
  number = {1},
  journal = {Journal of Diagnostic Medical Sonography},
  doi = {10.1177/875647939000600106},
  author = {Taylor, Richard},
  month = jan,
  year = {1990},
  pages = {35-39}
}

@article{chatterjee2007,
  title = {Generating {{Data}} with {{Identical Statistics}} but {{Dissimilar Graphics}}},
  volume = {61},
  issn = {0003-1305},
  abstract = {The Anscombe dataset is popular for teaching the importance of graphics in data analysis. It consists of four datasets that have identical summary statistics (e.g., mean, standard deviation, and correlation) but dissimilar data graphics (scatterplots). In this article, we provide a general procedure to generate datasets with identical summary statistics but dissimilar graphics by using a genetic algorithm based approach.},
  number = {3},
  journal = {The American Statistician},
  doi = {10.1198/000313007X220057},
  author = {Chatterjee, Sangit and Firat, Aykut},
  month = aug,
  year = {2007},
  keywords = {Genetic algorithms,Non-linear optimization,Ortho-normalization},
  pages = {248-254},
  file = {C:\\Users\\u0099946\\Zotero\\storage\\VKF3WZUB\\000313007X220057.html}
}

@inproceedings{matejka2017,
  address = {New York, NY, USA},
  series = {{{CHI}} '17},
  title = {Same {{Stats}}, {{Different Graphs}}: {{Generating Datasets}} with {{Varied Appearance}} and {{Identical Statistics Through Simulated Annealing}}},
  isbn = {978-1-4503-4655-9},
  shorttitle = {Same {{Stats}}, {{Different Graphs}}},
  abstract = {Datasets which are identical over a number of statistical properties, yet produce dissimilar graphs, are frequently used to illustrate the importance of graphical representations when exploring data. This paper presents a novel method for generating such datasets, along with several examples. Our technique varies from previous approaches in that new datasets are iteratively generated from a seed dataset through random perturbations of individual data points, and can be directed towards a desired outcome through a simulated annealing optimization strategy. Our method has the benefit of being agnostic to the particular statistical properties that are to remain constant between the datasets, and allows for control over the graphical appearance of resulting output.},
  booktitle = {Proceedings of the 2017 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  publisher = {{ACM}},
  doi = {10.1145/3025453.3025912},
  author = {Matejka, Justin and Fitzmaurice, George},
  year = {2017},
  keywords = {visualization,anscombe,scatter plots},
  pages = {1290--1294}
}

@article{anscombe1973,
  title = {Graphs in {{Statistical Analysis}}},
  volume = {27},
  issn = {0003-1305},
  number = {1},
  journal = {The American Statistician},
  doi = {10.1080/00031305.1973.10478966},
  author = {Anscombe, F. J.},
  month = feb,
  year = {1973},
  pages = {17-21},
  file = {C:\\Users\\u0099946\\Zotero\\storage\\8ZP5WEIR\\00031305.1973.html}
}

@article{abdullah1990,
  title = {On a {{Robust Correlation Coefficient}}},
  volume = {39},
  issn = {1467-9884},
  language = {en},
  number = {4},
  journal = {Journal of the Royal Statistical Society: Series D (The Statistician)},
  doi = {10.2307/2349088},
  author = {Abdullah, Mokhtar Bin},
  month = dec,
  year = {1990},
  pages = {455-460},
  file = {C:\\Users\\u0099946\\Zotero\\storage\\CE637VBE\\2349088.html}
}

@article{wicherts2016,
  title = {Degrees of {{Freedom}} in {{Planning}}, {{Running}}, {{Analyzing}}, and {{Reporting Psychological Studies}}: {{A Checklist}} to {{Avoid}} p-{{Hacking}}},
  volume = {7},
  issn = {1664-1078},
  shorttitle = {Degrees of {{Freedom}} in {{Planning}}, {{Running}}, {{Analyzing}}, and {{Reporting Psychological Studies}}},
  abstract = {The designing, collecting, analyzing, and reporting of psychological studies entail many choices that are often arbitrary. The opportunistic use of these so-called researcher degrees of freedom aimed at obtaining statistically significant results is problematic because it enhances the chances of false positive results and may inflate effect size estimates. In this review article, we present an extensive list of 34 degrees of freedom that researchers have in formulating hypotheses, and in designing, running, analyzing, and reporting of psychological research. The list can be used in research methods education, and as a checklist to assess the quality of preregistrations and to determine the potential for bias due to (arbitrary) choices in unregistered studies.},
  language = {English},
  journal = {Frontiers in Psychology},
  doi = {10.3389/fpsyg.2016.01832},
  author = {Wicherts, Jelte M. and Veldkamp, Coosje L. S. and Augusteijn, Hilde E. M. and Bakker, Marjan and {van Aert}, Robbie C. M. and {van Assen}, Marcel A. L. M.},
  year = {2016},
  keywords = {p-hacking,Significance testing,Bias,Experimental design (study designs),questionable research practices,Research methods education,significance chasing}
}

@article{wasserstein2016,
  title = {The {{ASA}}'s {{Statement}} on p-{{Values}}: {{Context}}, {{Process}}, and {{Purpose}}},
  volume = {70},
  issn = {0003-1305},
  shorttitle = {The {{ASA}}'s {{Statement}} on p-{{Values}}},
  number = {2},
  journal = {The American Statistician},
  doi = {10.1080/00031305.2016.1154108},
  author = {Wasserstein, Ronald L. and Lazar, Nicole A.},
  month = apr,
  year = {2016},
  pages = {129-133}
}

@misc{song2013,
  title = {Publication Bias: What Is It? {{How}} Do We Measure It? {{How}} Do We Avoid It?},
  shorttitle = {Publication Bias},
  abstract = {Publication bias: what is it? How do we measure it? How do we avoid it? Fujian Song, Lee Hooper, Yoon K LokeNorwich Medical School, University of East Anglia, Norwich, UKAbstract: Publication bias occurs when results of published studies are systematically different from results of unpublished studies. The term \&quot;dissemination bias\&quot; has also been recommended to describe all forms of biases in the research-dissemination process, including outcome-reporting bias, time-lag bias, gray-literature bias, full-publication bias, language bias, citation bias, and media-attention bias. We can measure publication bias by comparing the results of published and unpublished studies addressing the same question. Following up cohorts of studies from inception and comparing publication levels in studies with statistically significant or \&quot;positive\&quot; results suggested greater odds of formal publication in those with such results, compared to those without. Within reviews, funnel plots and related statistical methods can be used to indicate presence or absence of publication bias, although these can be unreliable in many circumstances. Methods of avoiding publication bias, by identifying and including unpublished outcomes and unpublished studies, are discussed and evaluated. These include searching without limiting by outcome, searching prospective trials registers, searching informal sources, including meeting abstracts and PhD theses, searching regulatory body websites, contacting authors of included studies, and contacting pharmaceutical or medical device companies for further studies. Adding unpublished studies often alters effect sizes, but may not always eliminate publication bias. The compulsory registration of all clinical trials at inception is an important move forward, but it can be difficult for reviewers to access data from unpublished studies located this way. Publication bias may be reduced by journals by publishing high-quality studies regardless of novelty or unexciting results, and by publishing protocols or full-study data sets. No single step can be relied upon to fully overcome the complex actions involved in publication bias, and a multipronged approach is required by researchers, patients, journal editors, peer reviewers, research sponsors, research ethics committees, and regulatory and legislation authorities.Keywords: publication bias, reporting bias, research-dissemination bias, evidence synthesis, systematic review, meta-analysis},
  language = {English},
  journal = {Open Access Journal of Clinical Trials},
  howpublished = {https://www.dovepress.com/publication-bias-what-is-it-how-do-we-measure-it-how-do-we-avoid-it-peer-reviewed-article-OAJCT},
  author = {Song, Fujian and Hooper, Lee and Loke, Yoon K.},
  month = jul,
  year = {2013},
  doi = {10.2147/OAJCT.S34419}
}

@article{lakens2017a,
  title = {Too {{True}} to Be {{Bad}}: {{When Sets}} of {{Studies With Significant}} and {{Nonsignificant Findings Are Probably True}}},
  volume = {8},
  issn = {1948-5506},
  shorttitle = {Too {{True}} to Be {{Bad}}},
  abstract = {Psychology journals rarely publish nonsignificant results. At the same time, it is often very unlikely (or ``too good to be true'') that a set of studies yields exclusively significant results. Here, we use likelihood ratios to explain when sets of studies that contain a mix of significant and nonsignificant results are likely to be true or ``too true to be bad.'' As we show, mixed results are not only likely to be observed in lines of research but also, when observed, often provide evidence for the alternative hypothesis, given reasonable levels of statistical power and an adequately controlled low Type 1 error rate. Researchers should feel comfortable submitting such lines of research with an internal meta-analysis for publication. A better understanding of probabilities, accompanied by more realistic expectations of what real sets of studies look like, might be an important step in mitigating publication bias in the scientific literature.},
  language = {en},
  number = {8},
  journal = {Social Psychological and Personality Science},
  doi = {10.1177/1948550617693058},
  author = {Lakens, Dani{\"e}l and Etz, Alexander J.},
  month = nov,
  year = {2017},
  pages = {875-881}
}

@article{martins2018,
  title = {Learning the Principles of Simulation Using the Birthday Problem},
  volume = {40},
  copyright = {\textcopyright{} 2018 Teaching Statistics Trust},
  issn = {1467-9639},
  abstract = {Using the famous Birthday problem, we present here a practical activity that allows students to perceive the basic reasoning behind simulation and explore its potential. Through a playful approach with probabilities, students are led along a path that illustrates difficulties with intuition and introduces them to theoretical results for sample proportions.},
  language = {en},
  number = {3},
  journal = {Teaching Statistics},
  doi = {10.1111/test.12164},
  author = {Martins, Rui Manuel da Costa},
  year = {2018},
  keywords = {Simulation,Birthday paradox,Football,Law of large numbers,Sample proportion,Teaching statistics},
  pages = {108-111}
}

@article{ohara2019,
  title = {Teaching Hypothesis Testing with Simulated Distributions},
  volume = {30},
  issn = {1477-3880},
  abstract = {I propose that econometrics instructors move away from the notion of critical values and tear out any remaining tables from the back of our textbooks. Instead, I propose that we teach students to test hypotheses by generating a simulated distribution of the test statistic under the null hypothesis using (psuedo-)random number generators. This is quick and easy to do using modern software packages, and provides students with a visual and intuitive understanding of sampling distributions and the logic behind hypothesis testing. I discuss what it is necessary to teach students to implement this approach, provide an example exercise, and discuss how to assess this understanding on exams when students do not have computer access.},
  journal = {International Review of Economics Education},
  doi = {10.1016/j.iree.2018.05.005},
  author = {O'Hara, Michael},
  month = jan,
  year = {2019},
  keywords = {Hypothesis testing,Simulation,Undergraduate teaching},
  pages = {100138}
}

@article{tintle2015,
  title = {Combating {{Anti}}-{{Statistical Thinking Using Simulation}}-{{Based Methods Throughout}} the {{Undergraduate Curriculum}}},
  volume = {69},
  issn = {0003-1305},
  abstract = {The use of simulation-based methods for introducing inferen-ce is growing in popularity for the Stat 101 course, due in part to increasing evidence of the methods ability to improve studen-ts' statistical thinking. This impact comes from simulation-based methods (a) clearly presenting the overarching logic of inference, (b) strengthening ties between statistics and probability/mathematical concepts, (c) encouraging a focus on the entire research process, (d) facilitating student thinking about advanced statistical concepts, (e) allowing more time to explore, do, and talk about real research and messy data, and (f) acting as a firm-er foundation on which to build statistical intuition. Thus, we argue that simulation-based inference should be an entry point to an undergraduate statistics program for all students, and that simulation-based inference should be used throughout all under-graduate statistics courses. To achieve this goal and fully recognize the benefits of simulation-based inference on the undergraduate statistics program, we will need to break free of historical forces tying undergraduate statistics curricula to mathematics, consider radical and innovative new pedagogical approaches in our courses, fully implement assessment-driven content innovations, and embrace computation throughout the curriculum.[Received December 2014. Revised July 2015]},
  number = {4},
  journal = {The American Statistician},
  doi = {10.1080/00031305.2015.1081619},
  author = {Tintle, Nathan and Chance, Beth and Cobb, George and Roy, Soma and Swanson, Todd and VanderStoep, Jill},
  month = oct,
  year = {2015},
  keywords = {Permutation,Education,Bootstrap,Randomization.},
  pages = {362-370}
}

@article{francis2013a,
  series = {Special {{Issue}}: {{A Discussion}} of {{Publication Bias}} and the {{Test}} for {{Excess Significance}}},
  title = {Replication, Statistical Consistency, and Publication Bias},
  volume = {57},
  issn = {0022-2496},
  abstract = {Scientific methods of investigation offer systematic ways to gather information about the world; and in the field of psychology application of such methods should lead to a better understanding of human behavior. Instead, recent reports in psychological science have used apparently scientific methods to report strong evidence for unbelievable claims such as precognition. To try to resolve the apparent conflict between unbelievable claims and the scientific method many researchers turn to empirical replication to reveal the truth. Such an approach relies on the belief that true phenomena can be successfully demonstrated in well-designed experiments, and the ability to reliably reproduce an experimental outcome is widely considered the gold standard of scientific investigations. Unfortunately, this view is incorrect; and misunderstandings about replication contribute to the conflicts in psychological science. Because experimental effects in psychology are measured by statistics, there should almost always be some variability in the reported outcomes. An absence of such variability actually indicates that experimental replications are invalid, perhaps because of a bias to suppress contrary findings or because the experiments were run improperly. Recent investigations have demonstrated how to identify evidence of such invalid experiment sets and noted its appearance for prominent findings in experimental psychology. The present manuscript explores those investigative methods by using computer simulations to demonstrate their properties and limitations. The methods are shown to be a check on the statistical consistency of a set of experiments by comparing the reported power of the experiments with the reported frequency of statistical significance. Overall, the methods are extremely conservative about reporting inconsistency when experiments are run properly and reported fully. The manuscript also considers how to improve scientific practice to avoid inconsistency, and discusses criticisms of the investigative method.},
  number = {5},
  journal = {Journal of Mathematical Psychology},
  doi = {10.1016/j.jmp.2013.02.003},
  author = {Francis, Gregory},
  month = oct,
  year = {2013},
  keywords = {Hypothesis testing,Publication bias,Scientific publishing,Statistics},
  pages = {153-169},
  file = {C:\\Users\\u0099946\\Zotero\\storage\\2RTVZ8EP\\S002224961300014X.html}
}

@article{kerr1998,
  title = {{{HARKing}}: Hypothesizing after the Results Are Known},
  volume = {2},
  issn = {1088-8683},
  shorttitle = {{{HARKing}}},
  abstract = {This article considers a practice in scientific communication termed HARKing (Hypothesizing After the Results are Known). HARKing is defined as presenting a post hoc hypothesis (i.e., one based on or informed by one's results) in one's research report as i f it were, in fact, an a priori hypotheses. Several forms of HARKing are identified and survey data are presented that suggests that at least some forms of HARKing are widely practiced and widely seen as inappropriate. I identify several reasons why scientists might HARK. Then I discuss several reasons why scientists ought not to HARK. It is conceded that the question of whether HARKing ' s costs exceed its benefits is a complex one that ought to be addressed through research, open discussion, and debate. To help stimulate such discussion (and for those such as myself who suspect that HARKing's costs do exceed its benefits), I conclude the article with some suggestions for deterring HARKing.},
  language = {eng},
  number = {3},
  journal = {Personality and Social Psychology Review: An Official Journal of the Society for Personality and Social Psychology, Inc},
  doi = {10.1207/s15327957pspr0203_4},
  author = {Kerr, N. L.},
  year = {1998},
  pages = {196-217},
  pmid = {15647155}
}

@article{nissen2016,
  title = {Publication Bias and the Canonization of False Facts},
  volume = {5},
  issn = {2050-084X},
  abstract = {Science is facing a ``replication crisis'' in which many experimental findings cannot be replicated and are likely to be false. Does this imply that many scientific facts are false as well? To find out, we explore the process by which a claim becomes fact. We model the community's confidence in a claim as a Markov process with successive published results shifting the degree of belief. Publication bias in favor of positive findings influences the distribution of published results. We find that unless a sufficient fraction of negative results are published, false claims frequently can become canonized as fact. Data-dredging, p-hacking, and similar behaviors exacerbate the problem. Should negative results become easier to publish as a claim approaches acceptance as a fact, however, true and false claims would be more readily distinguished. To the degree that the model reflects the real world, there may be serious concerns about the validity of purported facts in some disciplines.},
  journal = {eLife},
  doi = {10.7554/eLife.21451},
  author = {Nissen, Silas Boye and Magidson, Tali and Gross, Kevin and Bergstrom, Carl T},
  editor = {Rodgers, Peter},
  month = dec,
  year = {2016},
  keywords = {publication bias,hypothesis testing,replication crisis,false positive},
  pages = {e21451}
}

@article{smaldinopaule.,
  title = {The Natural Selection of Bad Science},
  volume = {3},
  abstract = {Poor research design and data analysis encourage false-positive findings. Such poor methods persist despite perennial calls for improvement, suggesting that they result from something more than just misunderstanding. The persistence of poor methods results partly from incentives that favour them, leading to the natural selection of bad science. This dynamic requires no conscious strategizing\textemdash{}no deliberate cheating nor loafing\textemdash{}by scientists, only that publication is a principal factor for career advancement. Some normative methods of analysis have almost certainly been selected to further publication instead of discovery. In order to improve the culture of science, a shift must be made away from correcting misunderstandings and towards rewarding understanding. We support this argument with empirical evidence and computational modelling. We first present a 60-year meta-analysis of statistical power in the behavioural sciences and show that power has not improved despite repeated demonstrations of the necessity of increasing power. To demonstrate the logical consequences of structural incentives, we then present a dynamic model of scientific communities in which competing laboratories investigate novel or previously published hypotheses using culturally transmitted research methods. As in the real world, successful labs produce more `progeny,' such that their methods are more often copied and their students are more likely to start labs of their own. Selection for high output leads to poorer methods and increasingly high false discovery rates. We additionally show that replication slows but does not stop the process of methodological deterioration. Improving the quality of research requires change at the institutional level.},
  number = {9},
  journal = {Royal Society Open Science},
  doi = {10.1098/rsos.160384},
  author = {{Smaldino Paul E.} and {McElreath Richard}},
  pages = {160384},
  file = {C:\\Users\\u0099946\\Zotero\\storage\\D4EEBVAT\\rsos.html}
}

@article{higginson2016a,
  title = {Current {{Incentives}} for {{Scientists Lead}} to {{Underpowered Studies}} with {{Erroneous Conclusions}}},
  volume = {14},
  issn = {1545-7885},
  abstract = {We can regard the wider incentive structures that operate across science, such as the priority given to novel findings, as an ecosystem within which scientists strive to maximise their fitness (i.e., publication record and career success). Here, we develop an optimality model that predicts the most rational research strategy, in terms of the proportion of research effort spent on seeking novel results rather than on confirmatory studies, and the amount of research effort per exploratory study. We show that, for parameter values derived from the scientific literature, researchers acting to maximise their fitness should spend most of their effort seeking novel results and conduct small studies that have only 10\%\textendash{}40\% statistical power. As a result, half of the studies they publish will report erroneous conclusions. Current incentive structures are in conflict with maximising the scientific value of research; we suggest ways that the scientific ecosystem could be improved.},
  language = {en},
  number = {11},
  journal = {PLOS Biology},
  doi = {10.1371/journal.pbio.2000995},
  author = {Higginson, Andrew D. and Munaf{\`o}, Marcus R.},
  month = nov,
  year = {2016},
  keywords = {Research assessment,Scientists,Careers,Careers in research,Drug discovery,Ecosystems,Peer review,Research errors},
  pages = {e2000995},
  file = {C:\\Users\\u0099946\\Zotero\\storage\\FAI69C2G\\article.html}
}

@article{lakens2018a,
  title = {Justify Your Alpha},
  volume = {2},
  copyright = {2018 The Publisher},
  issn = {2397-3374},
  abstract = {In response to recommendations to redefine statistical significance to P {$\leq$} 0.005, we propose that researchers should transparently report and justify all choices they make when designing a study, including the alpha level.},
  language = {En},
  number = {3},
  journal = {Nature Human Behaviour},
  doi = {10.1038/s41562-018-0311-x},
  author = {Lakens, Daniel and Adolfi, Federico G. and Albers, Casper J. and Anvari, Farid and Apps, Matthew A. J. and Argamon, Shlomo E. and Baguley, Thom and Becker, Raymond B. and Benning, Stephen D. and Bradford, Daniel E. and Buchanan, Erin M. and Caldwell, Aaron R. and Calster, Ben Van and Carlsson, Rickard and Chen, Sau-Chin and Chung, Bryan and Colling, Lincoln J. and Collins, Gary S. and Crook, Zander and Cross, Emily S. and Daniels, Sameera and Danielsson, Henrik and DeBruine, Lisa and Dunleavy, Daniel J. and Earp, Brian D. and Feist, Michele I. and Ferrell, Jason D. and Field, James G. and Fox, Nicholas W. and Friesen, Amanda and Gomes, Caio and {Gonzalez-Marquez}, Monica and Grange, James A. and Grieve, Andrew P. and Guggenberger, Robert and Grist, James and van Harmelen, Anne-Laura and Hasselman, Fred and Hochard, Kevin D. and Hoffarth, Mark R. and Holmes, Nicholas P. and Ingre, Michael and Isager, Peder M. and Isotalus, Hanna K. and Johansson, Christer and Juszczyk, Konrad and Kenny, David A. and Khalil, Ahmed A. and Konat, Barbara and Lao, Junpeng and Larsen, Erik Gahner and Lodder, Gerine M. A. and Lukavsk{\'y}, Ji{\v r}{\'i} and Madan, Christopher R. and Manheim, David and Martin, Stephen R. and Martin, Andrea E. and Mayo, Deborah G. and McCarthy, Randy J. and McConway, Kevin and McFarland, Colin and Nio, Amanda Q. X. and Nilsonne, Gustav and de Oliveira, Cilene Lino and de Xivry, Jean-Jacques Orban and Parsons, Sam and Pfuhl, Gerit and Quinn, Kimberly A. and Sakon, John J. and Saribay, S. Adil and Schneider, Iris K. and Selvaraju, Manojkumar and Sjoerds, Zsuzsika and Smith, Samuel G. and Smits, Tim and Spies, Jeffrey R. and Sreekumar, Vishnu and Steltenpohl, Crystal N. and Stenhouse, Neil and {\'S}wi{\k{a}}tkowski, Wojciech and Vadillo, Miguel A. and Assen, Marcel A. L. M. Van and Williams, Matt N. and Williams, Samantha E. and Williams, Donald R. and Yarkoni, Tal and Ziano, Ignazio and Zwaan, Rolf A.},
  month = mar,
  year = {2018},
  pages = {168},
  file = {C:\\Users\\u0099946\\Zotero\\storage\\IZXQX5TT\\Lakens et al. - 2018 - Justify your alpha.pdf;C:\\Users\\u0099946\\Zotero\\storage\\AUPCEEPL\\s41562-018-0311-x.html}
}

@article{amrhein2018a,
  title = {Remove, Rather than Redefine, Statistical Significance},
  volume = {2},
  copyright = {2017 The Publisher},
  issn = {2397-3374},
  abstract = {Correspondence},
  language = {En},
  number = {1},
  journal = {Nature Human Behaviour},
  doi = {10.1038/s41562-017-0224-0},
  author = {Amrhein, Valentin and Greenland, Sander},
  month = jan,
  year = {2018},
  pages = {4},
  file = {C:\\Users\\u0099946\\Zotero\\storage\\ZB7YF52W\\s41562-017-0224-0.html}
}

@article{benjamin2018,
  title = {Redefine Statistical Significance},
  volume = {2},
  copyright = {2017 The Author(s)},
  issn = {2397-3374},
  abstract = {We propose to change the default P-value threshold for statistical significance from 0.05 to 0.005 for claims of new discoveries.},
  language = {En},
  number = {1},
  journal = {Nature Human Behaviour},
  doi = {10.1038/s41562-017-0189-z},
  author = {Benjamin, Daniel J. and Berger, James O. and Johannesson, Magnus and Nosek, Brian A. and Wagenmakers, E.-J. and Berk, Richard and Bollen, Kenneth A. and Brembs, Bj{\"o}rn and Brown, Lawrence and Camerer, Colin and Cesarini, David and Chambers, Christopher D. and Clyde, Merlise and Cook, Thomas D. and Boeck, Paul De and Dienes, Zoltan and Dreber, Anna and Easwaran, Kenny and Efferson, Charles and Fehr, Ernst and Fidler, Fiona and Field, Andy P. and Forster, Malcolm and George, Edward I. and Gonzalez, Richard and Goodman, Steven and Green, Edwin and Green, Donald P. and Greenwald, Anthony G. and Hadfield, Jarrod D. and Hedges, Larry V. and Held, Leonhard and Ho, Teck Hua and Hoijtink, Herbert and Hruschka, Daniel J. and Imai, Kosuke and Imbens, Guido and Ioannidis, John P. A. and Jeon, Minjeong and Jones, James Holland and Kirchler, Michael and Laibson, David and List, John and Little, Roderick and Lupia, Arthur and Machery, Edouard and Maxwell, Scott E. and McCarthy, Michael and Moore, Don A. and Morgan, Stephen L. and Munaf{\'o}, Marcus and Nakagawa, Shinichi and Nyhan, Brendan and Parker, Timothy H. and Pericchi, Luis and Perugini, Marco and Rouder, Jeff and Rousseau, Judith and Savalei, Victoria and Sch{\"o}nbrodt, Felix D. and Sellke, Thomas and Sinclair, Betsy and Tingley, Dustin and Zandt, Trisha Van and Vazire, Simine and Watts, Duncan J. and Winship, Christopher and Wolpert, Robert L. and Xie, Yu and Young, Cristobal and Zinman, Jonathan and Johnson, Valen E.},
  month = jan,
  year = {2018},
  pages = {6},
  file = {C:\\Users\\u0099946\\Zotero\\storage\\BK3HIXH4\\Benjamin et al. - 2018 - Redefine statistical significance.pdf;C:\\Users\\u0099946\\Zotero\\storage\\7FGCSR68\\s41562-017-0189-z.html}
}

@article{szucs2017,
  title = {When {{Null Hypothesis Significance Testing Is Unsuitable}} for {{Research}}: {{A Reassessment}}},
  volume = {11},
  issn = {1662-5161},
  shorttitle = {When {{Null Hypothesis Significance Testing Is Unsuitable}} for {{Research}}},
  abstract = {Null hypothesis significance testing (NHST) has several shortcomings that are likely contributing factors behind the widely debated replication crisis of (cognitive) neuroscience, psychology and biomedical science in general. We review these shortcomings and suggest that, after about 60 years of negative experience, NHST should no longer be the default, dominant statistical practice of all biomedical and psychological research. If theoretical predictions are weak we should not rely on all or nothing hypothesis tests. Different inferential methods may be most suitable for different types of research questions. Whenever researchers use NHST they should justify its use, and publish pre-study power calculations and effect sizes, including negative findings. Hypothesis-testing studies should be pre-registered and optimally raw data published. The current statistics lite educational approach for students that has sustained the widespread, spurious use of NHST should be phased out. Instead, we should encourage either more in-depth statistical training of more researchers and/or more widespread involvement of professional statisticians in all research.},
  language = {English},
  journal = {Frontiers in Human Neuroscience},
  doi = {10.3389/fnhum.2017.00390},
  author = {Szucs, Denes and Ioannidis, John P. A.},
  year = {2017},
  keywords = {Null hypothesis significance testing,replication crisis,bayesian methods,False positive findings,Research Methodology},
  file = {C:\\Users\\u0099946\\Zotero\\storage\\6CZUA3EN\\Szucs and Ioannidis - 2017 - When Null Hypothesis Significance Testing Is Unsui.pdf}
}

@article{cook2019,
  title = {There Is Still a Place for Significance Testing in Clinical Trials},
  issn = {1740-7745},
  language = {en},
  journal = {Clinical Trials},
  doi = {10.1177/1740774519846504},
  author = {Cook, Jonathan A and Fergusson, Dean A and Ford, Ian and Gonen, Mithat and Kimmelman, Jonathan and Korn, Edward L and Begg, Colin B},
  month = may,
  year = {2019},
  pages = {1740774519846504},
  file = {C:\\Users\\u0099946\\Zotero\\storage\\NHXHQC2Z\\Cook et al. - 2019 - There is still a place for significance testing in.pdf}
}

@article{kmetz2019,
  title = {Correcting {{Corrupt Research}}: {{Recommendations}} for the {{Profession}} to {{Stop Misuse}} of p-{{Values}}},
  volume = {73},
  issn = {0003-1305},
  shorttitle = {Correcting {{Corrupt Research}}},
  abstract = {p-Values and Null Hypothesis Significance Testing (NHST), combined with a large number of institutional factors, jointly define the Generally Accepted Soft Social Science Publishing Process (GASSSPP) that is now dominant in the social sciences and is increasingly used elsewhere. The case against NHST and the GASSSPP has been abundantly articulated over past decades, and yet it continues to spread, supported by a large number of self-reinforcing institutional processes. In this article, the author presents a number of steps that may be taken to counter the spread of this corruption that directly address the institutional forces, both as individuals and through collaborative efforts. While individual efforts are indispensable to this undertaking, the author argues that these alone cannot succeed unless the institutional forces are also addressed. Supplementary materials for this article are available online.},
  number = {sup1},
  journal = {The American Statistician},
  doi = {10.1080/00031305.2018.1518271},
  author = {Kmetz, John L.},
  month = mar,
  year = {2019},
  keywords = {Peer review,Corrupt research,Generally Accepted Soft-Social-Science Publishing Process (GASSSPP),p-Values,Reform,Replication},
  pages = {36-45},
  file = {C:\\Users\\u0099946\\Zotero\\storage\\VC77RBA9\\Kmetz - 2019 - Correcting Corrupt Research Recommendations for t.pdf;C:\\Users\\u0099946\\Zotero\\storage\\9Z5JYVMZ\\00031305.2018.html}
}

@article{krueger2019,
  title = {Putting the {{P}}-{{Value}} in Its {{Place}}},
  volume = {73},
  issn = {0003-1305},
  abstract = {As the debate over best statistical practices continues in academic journals, conferences, and the blogosphere, working researchers (e.g., psychologists) need to figure out how much time and effort to invest in attending to experts' arguments, how to design their next project, and how to craft a sustainable long-term strategy for data analysis and inference. The present special issue of The American Statistician promises help. In this article, we offer a modest proposal for a continued and informed use of the conventional p-value without the pitfalls of statistical rituals. Other statistical indices should complement reporting, and extra-statistical (e.g., theoretical) judgments ought to be made with care and clarity.},
  number = {sup1},
  journal = {The American Statistician},
  doi = {10.1080/00031305.2018.1470033},
  author = {Krueger, Joachim I. and Heck, Patrick R.},
  month = mar,
  year = {2019},
  keywords = {Bayes' theorem,Inference,Null hypotheses,p-values,Statistical significance testing},
  pages = {122-128},
  file = {C:\\Users\\u0099946\\Zotero\\storage\\LBRMAZEQ\\Krueger and Heck - 2019 - Putting the P-Value in its Place.pdf;C:\\Users\\u0099946\\Zotero\\storage\\7C5JKXHK\\00031305.2018.html}
}

@techreport{lakens2019,
  type = {Preprint},
  title = {The Practical Alternative to the P-Value Is the Correctly Used p-Value},
  abstract = {Due to the strong overreliance on p-values in the scientific literature some researchers have argued that p-values should be abandoned or banned, and that we need to move beyond p-values and embrace practical alternatives. When proposing alternatives to p-values statisticians often commit the `Statistician's Fallacy', where they declare which statistic researchers really `want to know'. Instead of telling researchers what they want to know, statisticians should teach researchers which questions they can ask. In some situations, the answer to the question they are most interested in will be the p-value. There is clear room for improvement in how we teach p-values. If anyone really believes p-values are an important cause of problems in science, preventing the misinterpretation of p-values by developing better evidence-based education and user-centered statistical software should be a top priority. At least for some research questions we might not need alternatives to p-values, but can improve our inferences more by using alternatives to null-hypothesis tests. As long as null-hypothesis tests have been criticized, researchers have suggested to include minimal-effects tests and equivalence tests in our statistical toolbox, and these tests have the potential to greatly improve the questions researchers ask.},
  institution = {{PsyArXiv}},
  author = {Lakens, Daniel},
  month = apr,
  year = {2019},
  doi = {10.31234/osf.io/shm8v}
}

@article{jr2019,
  title = {Assessing the {{Statistical Analyses Used}} in {{Basic}} and {{Applied Social Psychology After Their}} P-{{Value Ban}}},
  volume = {73},
  issn = {0003-1305},
  abstract = {In this article, we assess the 31 articles published in Basic and Applied Social Psychology (BASP) in 2016, which is one full year after the BASP editors banned the use of inferential statistics. We discuss how the authors collected their data, how they reported and summarized their data, and how they used their data to reach conclusions. We found multiple instances of authors overstating conclusions beyond what the data would support if statistical significance had been considered. Readers would be largely unable to recognize this because the necessary information to do so was not readily available.},
  number = {sup1},
  journal = {The American Statistician},
  doi = {10.1080/00031305.2018.1537892},
  author = {Jr, Ronald D. Fricker and Burke, Katherine and Han, Xiaoyan and Woodall, William H.},
  month = mar,
  year = {2019},
  keywords = {Psychology,Statistical significance,Effect size,Inference ban,NHST},
  pages = {374-384},
  file = {C:\\Users\\u0099946\\Zotero\\storage\\HWMUP72W\\Jr et al. - 2019 - Assessing the Statistical Analyses Used in Basic a.pdf;C:\\Users\\u0099946\\Zotero\\storage\\METQ76KB\\00031305.2018.html}
}

@article{hubbard2008,
  title = {Why {{P Values Are Not}} a {{Useful Measure}} of {{Evidence}} in {{Statistical Significance Testing}}},
  volume = {18},
  issn = {0959-3543},
  abstract = {Reporting p values from statistical significance tests is common in psychology's empirical literature. Sir Ronald Fisher saw the p value as playing a useful role in knowledge development by acting as an `objective' measure of inductive evidence against the null hypothesis. We review several reasons why the p value is an unobjective and inadequate measure of evidence when statistically testing hypotheses. A common theme throughout many of these reasons is that p values exaggerate the evidence against H0. This, in turn, calls into question the validity of much published work based on comparatively small, including .05, p values. Indeed, if researchers were fully informed about the limitations of the  p value as a measure of evidence, this inferential index could not possibly enjoy its ongoing ubiquity. Replication with extension research focusing on sample statistics, effect sizes, and their confidence intervals is a better vehicle for reliable knowledge development than using p values. Fisher would also have agreed with the need for replication research.},
  language = {en},
  number = {1},
  journal = {Theory \& Psychology},
  doi = {10.1177/0959354307086923},
  author = {Hubbard, Raymond and Lindsay, R. Murray},
  month = feb,
  year = {2008},
  pages = {69-88}
}

@article{wagenmakers2007,
  title = {A Practical Solution to the Pervasive Problems Ofp Values},
  volume = {14},
  issn = {1531-5320},
  abstract = {In the field of psychology, the practice ofp value null-hypothesis testing is as widespread as ever. Despite this popularity, or perhaps because of it, most psychologists are not aware of the statistical peculiarities of thep value procedure. In particular,p values are based on data that were never observed, and these hypothetical data are themselves influenced by subjective intentions. Moreover,p values do not quantify statistical evidence. This article reviews thesep value problems and illustrates each problem with concrete examples. The three problems are familiar to statisticians but may be new to psychologists. A practical solution to thesep value problems is to adopt a model selection perspective and use the Bayesian information criterion (BIC) for statistical inference (Raftery, 1995). The BIC provides an approximation to a Bayesian hypothesis test, does not require the specification of priors, and can be easily calculated from SPSS output.},
  language = {en},
  number = {5},
  journal = {Psychonomic Bulletin \& Review},
  doi = {10.3758/BF03194105},
  author = {Wagenmakers, Eric-Jan},
  month = oct,
  year = {2007},
  keywords = {Bayesian Information Criterion,Null Hypothesis,Posterior Probability,Prior Distribution,Statistical Inference},
  pages = {779-804},
  file = {C:\\Users\\u0099946\\Zotero\\storage\\62NK6AY6\\Wagenmakers - 2007 - A practical solution to the pervasive problems ofp.pdf}
}

@article{sullivan2012,
  title = {Using {{Effect Size}}\textemdash{}or {{Why}} the {{P Value Is Not Enough}}},
  volume = {4},
  issn = {1949-8349},
  number = {3},
  journal = {Journal of Graduate Medical Education},
  doi = {10.4300/JGME-D-12-00156.1},
  author = {Sullivan, Gail M. and Feinn, Richard},
  month = sep,
  year = {2012},
  pages = {279-282},
  file = {C:\\Users\\u0099946\\Zotero\\storage\\W6BFEI48\\Sullivan and Feinn - 2012 - Using Effect Sizeor Why the P Value Is Not Enough.pdf},
  pmid = {23997866},
  pmcid = {PMC3444174}
}

@article{sutton2000,
  title = {Empirical Assessment of Effect of Publication Bias on Meta-Analyses},
  volume = {320},
  issn = {0959-8138},
  abstract = {Objective
To assess the effect of publication bias on the results and conclusions of systematic reviews and meta-analyses.

Design
Analysis of published meta-analyses by trim and fill method.

Studies
48 reviews in Cochrane Database of Systematic Reviews that considered a binary endpoint and contained 10 or more individual studies.

Main outcome measures
Number of reviews with missing studies and effect on conclusions of meta-analyses.

Results
The trim and fill fixed effects analysis method estimated that 26 (54\%) of reviews had missing studies and in 10 the number missing was significant. The corresponding figures with a random effects model were 23 (48\%) and eight. In four cases, statistical inferences regarding the effect of the intervention were changed after the overall estimate for publication bias was adjusted for.

Conclusions
Publication or related biases were common within the sample of meta-analyses assessed. In most cases these biases did not affect the conclusions. Nevertheless, researchers should check routinely whether conclusions of systematic reviews are robust to possible non-random selection mechanisms.},
  number = {7249},
  journal = {BMJ : British Medical Journal},
  author = {Sutton, A J and Duval, S J and Tweedie, R L and Abrams, K R and Jones, D R},
  month = jun,
  year = {2000},
  pages = {1574-1577},
  file = {C:\\Users\\u0099946\\Zotero\\storage\\84Q8CZ4P\\Sutton et al. - 2000 - Empirical assessment of effect of publication bias.pdf},
  pmid = {10845965},
  pmcid = {PMC27401}
}

@book{book,
  title = {What Is a P-Value Anyway? 34 {{Stories}} to {{Help You Actually Understand Statistics}}},
  isbn = {0-321-62930-2},
  author = {J. Vickers, Andrew},
  month = jan,
  year = {2009}
}

@article{zimmerman1994,
  title = {A {{Note}} on the {{Influence}} of {{Outliers}} on {{Parametric}} and {{Nonparametric Tests}}},
  volume = {121},
  issn = {0022-1309},
  abstract = {Extremely deviant scores, or outliers, reduce the probability of Type I errors of the Student t test and, at the same time, substantially increase the probability of Type II errors, so that power declines. The magnitude of the change depends jointly on the probability of occurrence of an outlier and its extremity, or its distance from the mean. Although outliers do not modify the probability of Type I errors of the Mann-Whitney-Wilcoxon test, they nevertheless increase the probability of Type II errors and reduce power. The effect on this nonparametric test depends largely on the probability of occurrence and not the extremity. Because deviant scores influence the t test to a relatively greater extent, the nonparametric method acquires an advantage for outlier-prone densities despite its loss of power.},
  number = {4},
  journal = {The Journal of General Psychology},
  doi = {10.1080/00221309.1994.9921213},
  author = {Zimmerman, Donald W.},
  month = oct,
  year = {1994},
  pages = {391-401},
  file = {C:\\Users\\u0099946\\Zotero\\storage\\2FJNKWRW\\00221309.1994.html}
}

@article{benjamini1995controlling,
  title = {Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing},
  volume = {57},
  number = {1},
  journal = {Journal of the Royal statistical society: series B (Methodological)},
  author = {Benjamini, Yoav and Hochberg, Yosef},
  year = {1995},
  pages = {289-300},
  publisher = {{Wiley Online Library}}
}

@article{rousselet2012a,
  title = {Improving Standards in Brain-Behavior Correlation Analyses},
  volume = {6},
  issn = {1662-5161},
  abstract = {Associations between two variables, for instance between brain and behavioural measurements, are often studied using Pearson correlation. However, Pearson correlation is not robust: outliers can introduce false correlations or mask existing ones. These problems are exacerbated in brain imaging by a widespread lack of control for multiple comparisons, and several issues with data interpretations. We illustrate these important problems associated with brain-behaviour correlations, drawing examples from published articles in mainstream high-impact and specialty journals. We make several propositions to alleviate these problems.},
  language = {English},
  journal = {Frontiers in Human Neuroscience},
  doi = {10.3389/fnhum.2012.00119},
  author = {Rousselet, Guillaume A. and Pernet, Cyril R.},
  year = {2012},
  keywords = {confidence intervals,multiple comparisons,multivariate statistics,outliers,Pearson correlation,robust statistics,skipped correlation,Spearman correlation},
  file = {C:\\Users\\u0099946\\Zotero\\storage\\QUMUNFR3\\Rousselet and Pernet - 2012 - Improving standards in brain-behavior correlation .pdf}
}

@article{schimmack2012a,
  title = {The Ironic Effect of Significant Results on the Credibility of Multiple-Study Articles},
  volume = {17},
  issn = {1939-1463},
  abstract = {Cohen (1962) pointed out the importance of statistical power for psychology as a science, but statistical power of studies has not increased, while the number of studies in a single article has increased. It has been overlooked that multiple studies with modest power have a high probability of producing nonsignificant results because power decreases as a function of the number of statistical tests that are being conducted (Maxwell, 2004). The discrepancy between the expected number of significant results and the actual number of significant results in multiple-study articles undermines the credibility of the reported results, and it is likely that questionable research practices have contributed to the reporting of too many significant results (Sterling, 1959). The problem of low power in multiple-study articles is illustrated using Bem's (2011) article on extrasensory perception and Gailliot et al.'s (2007) article on glucose and self-regulation. I conclude with several recommendations that can increase the credibility of scientific evidence in psychological journals. One major recommendation is to pay more attention to the power of studies to produce positive results without the help of questionable research practices and to request that authors justify sample sizes with a priori predictions of effect sizes. It is also important to publish replication studies with nonsignificant results if these studies have high power to replicate a published finding.},
  language = {eng},
  number = {4},
  journal = {Psychological Methods},
  doi = {10.1037/a0029487},
  author = {Schimmack, Ulrich},
  month = dec,
  year = {2012},
  keywords = {Humans,Psychology,Publication Bias,Research Design,Sample Size,Statistics as Topic},
  pages = {551-566},
  file = {C:\\Users\\u0099946\\Zotero\\storage\\PD3Z6IJL\\Schimmack - 2012 - The ironic effect of significant results on the cr.pdf},
  pmid = {22924598}
}

@article{rousselet2012b,
  title = {Improving Standards in Brain-Behavior Correlation Analyses},
  volume = {6},
  issn = {1662-5161},
  abstract = {Associations between two variables, for instance between brain and behavioural measurements, are often studied using Pearson correlation. However, Pearson correlation is not robust: outliers can introduce false correlations or mask existing ones. These problems are exacerbated in brain imaging by a widespread lack of control for multiple comparisons, and several issues with data interpretations. We illustrate these important problems associated with brain-behaviour correlations, drawing examples from published articles in mainstream high-impact and specialty journals. We make several propositions to alleviate these problems.},
  language = {English},
  journal = {Frontiers in Human Neuroscience},
  doi = {10.3389/fnhum.2012.00119},
  author = {Rousselet, Guillaume A. and Pernet, Cyril R.},
  year = {2012},
  keywords = {confidence intervals,multiple comparisons,multivariate statistics,outliers,Pearson correlation,robust statistics,skipped correlation,Spearman correlation},
  file = {C:\\Users\\u0099946\\Zotero\\storage\\D5ALSSEZ\\Rousselet and Pernet - 2012 - Improving standards in brain-behavior correlation .pdf}
}

@article{yarkoni2009,
  title = {Big {{Correlations}} in {{Little Studies}}: {{Inflated fMRI Correlations Reflect Low Statistical Power}}\textemdash{{Commentary}} on {{Vul}} et Al. (2009)},
  volume = {4},
  issn = {1745-6916},
  shorttitle = {Big {{Correlations}} in {{Little Studies}}},
  abstract = {Vul, Harris, Winkielman, and Pashler (2009), (this issue) argue that correlations in many cognitive neuroscience studies are grossly inflated due to a widespread tendency to use nonindependent analyses. In this article, I argue that Vul et al.'s primary conclusion is correct, but for different reasons than they suggest. I demonstrate that the primary cause of grossly inflated correlations in whole-brain fMRI analyses is not nonindependence, but the pernicious combination of small sample sizes and stringent alpha-correction levels. Far from defusing Vul et al.'s conclusions, the simulations presented suggest that the level of inflation may be even worse than Vul et al.'s empirical analysis would suggest.},
  language = {en},
  number = {3},
  journal = {Perspectives on Psychological Science},
  doi = {10.1111/j.1745-6924.2009.01127.x},
  author = {Yarkoni, Tal},
  month = may,
  year = {2009},
  pages = {294-298},
  file = {C:\\Users\\u0099946\\Zotero\\storage\\2YTJA68R\\Yarkoni - 2009 - Big Correlations in Little Studies Inflated fMRI .pdf}
}

@article{ioannidis2008,
  title = {Why {{Most Discovered True Associations Are Inflated}}},
  volume = {19},
  issn = {1044-3983},
  abstract = {Newly discovered true (non-null) associations often have inflated effects compared with the true effect sizes. I discuss here the main reasons for this inflation. First, theoretical considerations prove that when true discovery is claimed based on crossing a threshold of statistical significance and the discovery study is underpowered, the observed effects are expected to be inflated. This has been demonstrated in various fields ranging from early stopped clinical trials to genome-wide associations. Second, flexible analyses coupled with selective reporting may inflate the published discovered effects. The vibration ratio (the ratio of the largest vs. smallest effect on the same association approached with different analytic choices) can be very large. Third, effects may be inflated at the stage of interpretation due to diverse conflicts of interest. Discovered effects are not always inflated, and under some circumstances may be deflated\textemdash{}for example, in the setting of late discovery of associations in sequentially accumulated overpowered evidence, in some types of misclassification from measurement error, and in conflicts causing reverse biases. Finally, I discuss potential approaches to this problem. These include being cautious about newly discovered effect sizes, considering some rational down-adjustment, using analytical methods that correct for the anticipated inflation, ignoring the magnitude of the effect (if not necessary), conducting large studies in the discovery phase, using strict protocols for analyses, pursuing complete and transparent reporting of all results, placing emphasis on replication, and being fair with interpretation of results.},
  language = {en-US},
  number = {5},
  journal = {Epidemiology},
  doi = {10.1097/EDE.0b013e31818131e7},
  author = {Ioannidis, John P. A.},
  month = sep,
  year = {2008},
  pages = {640},
  file = {C:\\Users\\u0099946\\Zotero\\storage\\QRKSU72L\\Why_Most_Discovered_True_Associations_Are_Inflated.2.html}
}

@article{lakens2017b,
  title = {Equivalence {{Tests}}: {{A Practical Primer}} for t {{Tests}}, {{Correlations}}, and {{Meta}}-{{Analyses}}},
  volume = {8},
  issn = {1948-5506},
  shorttitle = {Equivalence {{Tests}}},
  abstract = {Scientists should be able to provide support for the absence of a meaningful effect. Currently, researchers often incorrectly conclude an effect is absent based a nonsignificant result. A widely recommended approach within a frequentist framework is to test for equivalence. In equivalence tests, such as the two one-sided tests (TOST) procedure discussed in this article, an upper and lower equivalence bound is specified based on the smallest effect size of interest. The TOST procedure can be used to statistically reject the presence of effects large enough to be considered worthwhile. This practical primer with accompanying spreadsheet and R package enables psychologists to easily perform equivalence tests (and power analyses) by setting equivalence bounds based on standardized effect sizes and provides recommendations to prespecify equivalence bounds. Extending your statistical tool kit with equivalence tests is an easy way to improve your statistical and theoretical inferences.},
  language = {en},
  number = {4},
  journal = {Social Psychological and Personality Science},
  doi = {10.1177/1948550617697177},
  author = {Lakens, Dani{\"e}l},
  month = may,
  year = {2017},
  pages = {355-362},
  file = {C:\\Users\\u0099946\\Zotero\\storage\\782896GC\\Lakens - 2017 - Equivalence Tests A Practical Primer for t Tests,.pdf}
}

@article{goertzen2010,
  title = {Detecting a Lack of Association: {{An}} Equivalence Testing Approach},
  volume = {63},
  issn = {2044-8317},
  shorttitle = {Detecting a Lack of Association},
  language = {en},
  number = {3},
  journal = {British Journal of Mathematical and Statistical Psychology},
  doi = {10.1348/000711009X475853},
  author = {Goertzen, Jason R. and Cribbie, Robert A.},
  month = nov,
  year = {2010},
  pages = {527-537},
  file = {C:\\Users\\u0099946\\Zotero\\storage\\HGPE3ZC5\\Goertzen and Cribbie - 2010 - Detecting a lack of association An equivalence te.pdf;C:\\Users\\u0099946\\Zotero\\storage\\PSV2GST2\\000711009X475853.html}
}

@article{robinson2005,
  title = {A Regression-Based Equivalence Test for Model Validation: Shifting the Burden of Proof},
  volume = {25},
  issn = {0829-318X},
  shorttitle = {A Regression-Based Equivalence Test for Model Validation},
  abstract = {Abstract.  Model validation is often realized as a test of how well model predictions match a set of independent observations. One would think that the burden o},
  language = {en},
  number = {7},
  journal = {Tree Physiology},
  doi = {10.1093/treephys/25.7.903},
  author = {Robinson, Andrew P. and Duursma, Remko A. and Marshall, John D.},
  month = jul,
  year = {2005},
  pages = {903-913},
  file = {C:\\Users\\u0099946\\Zotero\\storage\\KUP6UGD6\\Robinson et al. - 2005 - A regression-based equivalence test for model vali.pdf;C:\\Users\\u0099946\\Zotero\\storage\\TEKJZASM\\1673671.html}
}

@article{counsell2015,
  title = {Equivalence Tests for Comparing Correlation and Regression Coefficients},
  volume = {68},
  copyright = {\textcopyright{} 2014 The British Psychological Society},
  issn = {2044-8317},
  abstract = {Equivalence tests are an alternative to traditional difference-based tests for demonstrating a lack of association between two variables. While there are several recent studies investigating equivalence tests for comparing means, little research has been conducted on equivalence methods for evaluating the equivalence or similarity of two correlation coefficients or two regression coefficients. The current project proposes novel tests for evaluating the equivalence of two regression or correlation coefficients derived from the two one-sided tests (TOST) method (Schuirmann, 1987, J. Pharmacokinet. Biopharm, 15, 657) and an equivalence test by Anderson and Hauck (1983, Stat. Commun., 12, 2663). A simulation study was used to evaluate the performance of these tests and compare them with the common, yet inappropriate, method of assessing equivalence using non-rejection of the null hypothesis in difference-based tests. Results demonstrate that equivalence tests have more accurate probabilities of declaring equivalence than difference-based tests. However, equivalence tests require large sample sizes to ensure adequate power. We recommend the Anderson\textendash{}Hauck equivalence test over the TOST method for comparing correlation or regression coefficients.},
  language = {en},
  number = {2},
  journal = {British Journal of Mathematical and Statistical Psychology},
  doi = {10.1111/bmsp.12045},
  author = {Counsell, Alyssa and Cribbie, Robert A.},
  year = {2015},
  keywords = {regression,correlation,equivalence testing},
  pages = {292-309},
  file = {J:\\GBW-0434_GroupOrban\\JJ\\Biblio\\2015\\Counsell, Cribbie - 2015 - Equivalence tests for comparing correlation and regression coefficients.pdf}
}

@article{tversky1971,
  title = {Belief in the Law of Small Numbers},
  abstract = {People have erroneous intuitions about the laws of chance. In particular, they regard a sample randomly drawn from a population as highly representative, that is, similar to the population in all essential characteristics. The prevalence of the belief and its unfortunate consequences for psychological research are illustrated by the responses of professional psychologists to a questionnaire concerning research decisions. Apparently, most psychologists have an ex-aggerated belief in the likelihood of successfully replicating an obtained finding. The sources of such beliefs, and their consequences for the conduct of scientific inquiry, are what this paper is about. Our thesis is that people have strong intuitions about random sampling; that these intuitions are wrong in fundamental respects; that these intuitions are},
  journal = {Psychological Bulletin},
  author = {Tversky, Amos and Kahneman, Daniel},
  year = {1971},
  pages = {105--110},
  file = {J:\\GBW-0434_GroupOrban\\JJ\\Biblio\\1971\\Tversky, Kahneman - 1971 - Belief in the law of small numbers.pdf}
}

@article{calin-jageman2019,
  title = {The {{New Statistics}} for {{Better Science}}: {{Ask How Much}}, {{How Uncertain}}, and {{What Else Is Known}}},
  volume = {73},
  issn = {0003-1305},
  shorttitle = {The {{New Statistics}} for {{Better Science}}},
  abstract = {The ``New Statistics'' emphasizes effect sizes, confidence intervals, meta-analysis, and the use of Open Science practices. We present three specific ways in which a New Statistics approach can help improve scientific practice: by reducing overconfidence in small samples, by reducing confirmation bias, and by fostering more cautious judgments of consistency. We illustrate these points through consideration of the literature on oxytocin and human trust, a research area that typifies some of the endemic problems that arise with poor statistical practice.},
  number = {sup1},
  journal = {The American Statistician},
  doi = {10.1080/00031305.2018.1518266},
  author = {{Calin-Jageman}, Robert J. and Cumming, Geoff},
  month = mar,
  year = {2019},
  keywords = {Confidence intervals,Open science,Estimation,Meta-Analysis,The New Statistics},
  pages = {271-280},
  file = {J:\\GBW-0434_GroupOrban\\JJ\\Biblio\\2019\\Calin-Jageman, Cumming - 2019 - The New Statistics for Better Science.pdf}
}

@article{gelman2006b,
  title = {The {{Difference Between}} ``{{Significant}}'' and ``{{Not Significant}}'' Is Not {{Itself Statistically Significant}}},
  volume = {60},
  issn = {0003-1305},
  abstract = {It is common to summarize statistical comparisons by declarations of statistical significance or nonsignificance. Here we discuss one problem with such declarations, namely that changes in statistical significance are often not themselves statistically significant. By this, we are not merely making the commonplace observation that any particular threshold is arbitrary\textemdash{}for example, only a small change is required to move an estimate from a 5.1\% significance level to 4.9\%, thus moving it into statistical significance. Rather, we are pointing out that even large changes in significance levels can correspond to small, nonsignificant changes in the underlying quantities.The error we describe is conceptually different from other oft-cited problems\textemdash{}that statistical significance is not the same as practical importance, that dichotomization into significant and nonsignificant results encourages the dismissal of observed differences in favor of the usually less interesting null hypothesis of no difference, and that any particular threshold for declaring significance is arbitrary. We are troubled by all of these concerns and do not intend to minimize their importance. Rather, our goal is to bring attention to this additional error of interpretation. We illustrate with a theoretical example and two applied examples. The ubiquity of this statistical error leads us to suggest that students and practitioners be made more aware that the difference between ``significant'' and ``not significant'' is not itself statistically significant.},
  number = {4},
  journal = {The American Statistician},
  doi = {10.1198/000313006X152649},
  author = {Gelman, Andrew and Stern, Hal},
  month = nov,
  year = {2006},
  keywords = {Meta-analysis,Hypothesis testing,Replication,Pairwise comparison},
  pages = {328-331},
  file = {J:\\GBW-0434_GroupOrban\\JJ\\Biblio\\2006\\Gelman, Stern - 2006 - The Difference Between Significant and Not Significant is not Itself.pdf}
}

@article{akker2018,
  title = {How Do Academics Assess the Results of Multiple Experiments?},
  abstract = {We studied how academics assess the results of a set of four experiments that all test a given theory. We found that participants' belief in the theory increases with the number of significant results, and that direct replications were considered to be more important than conceptual replications. We found no difference between authors and reviewers in their propensity to submit or recommend to publish sets of results, but we did find that authors are generally more likely to desire an additional experiment. In a preregistered secondary analysis of individual participant data, we examined the heuristics academics use to assess the results of four experiments. Only 6 out of 312 (1.9\%) participants we analyzed used the normative method of Bayesian inference, whereas the majority of participants used vote counting approaches that tend to undervalue the evidence for the underlying theory if two or more results are statistically significant.},
  doi = {10.31234/osf.io/xyks4},
  author = {van den Akker, Olmo and Alvarez, Linda Dominguez and Bakker, Marjan and Wicherts, Jelte and van Assen, Marcel A. L. M.},
  month = dec,
  year = {2018},
  file = {J:\\GBW-0434_GroupOrban\\JJ\\Biblio\\2018\\Akker et al. - 2018 - How do academics assess the results of multiple experiments.pdf}
}

@article{gigerenzer2018,
  title = {Statistical {{Rituals}}: {{The Replication Delusion}} and {{How We Got There}}},
  volume = {1},
  issn = {2515-2459},
  shorttitle = {Statistical {{Rituals}}},
  abstract = {The ``replication crisis'' has been attributed to misguided external incentives gamed by researchers (the strategic-game hypothesis). Here, I want to draw attention to a complementary internal factor, namely, researchers' widespread faith in a statistical ritual and associated delusions (the statistical-ritual hypothesis). The ``null ritual,'' unknown in statistics proper, eliminates judgment precisely at points where statistical theories demand it. The crucial delusion is that the p value specifies the probability of a successful replication (i.e., 1 \textendash{} p), which makes replication studies appear to be superfluous. A review of studies with 839 academic psychologists and 991 students shows that the replication delusion existed among 20\% of the faculty teaching statistics in psychology, 39\% of the professors and lecturers, and 66\% of the students. Two further beliefs, the illusion of certainty (e.g., that statistical significance proves that an effect exists) and Bayesian wishful thinking (e.g., that the probability of the alternative hypothesis being true is 1 \textendash{} p), also make successful replication appear to be certain or almost certain, respectively. In every study reviewed, the majority of researchers (56\%\textendash{}97\%) exhibited one or more of these delusions. Psychology departments need to begin teaching statistical thinking, not rituals, and journal editors should no longer accept manuscripts that report results as ``significant'' or ``not significant.''},
  language = {en},
  number = {2},
  journal = {Advances in Methods and Practices in Psychological Science},
  doi = {10.1177/2515245918771329},
  author = {Gigerenzer, Gerd},
  month = jun,
  year = {2018},
  pages = {198-218},
  file = {C:\\Users\\u0099946\\Zotero\\storage\\TPNCTYUK\\Gigerenzer - 2018 - Statistical Rituals The Replication Delusion and .pdf}
}

@article{mcshane2015,
  title = {Blinding {{Us}} to the {{Obvious}}? {{The Effect}} of {{Statistical Training}} on the {{Evaluation}} of {{Evidence}}},
  volume = {62},
  issn = {0025-1909},
  shorttitle = {Blinding {{Us}} to the {{Obvious}}?},
  abstract = {Statistical training helps individuals analyze and interpret data. However, the emphasis placed on null hypothesis significance testing in academic training and reporting may lead researchers to interpret evidence dichotomously rather than continuously. Consequently, researchers may either disregard evidence that fails to attain statistical significance or undervalue it relative to evidence that attains statistical significance. Surveys of researchers across a wide variety of fields (including medicine, epidemiology, cognitive science, psychology, business, and economics) show that a substantial majority does indeed do so. This phenomenon is manifest both in researchers' interpretations of descriptions of evidence and in their likelihood judgments. Dichotomization of evidence is reduced though still present when researchers are asked to make decisions based on the evidence, particularly when the decision outcome is personally consequential. Recommendations are offered.This paper was accepted by Yuval Rottenstreich, judgment and decision making.},
  number = {6},
  journal = {Management Science},
  doi = {10.1287/mnsc.2015.2212},
  author = {McShane, Blakeley B. and Gal, David},
  month = sep,
  year = {2015},
  pages = {1707-1718},
  file = {J:\\GBW-0434_GroupOrban\\JJ\\Biblio\\2015\\McShane, Gal - 2015 - Blinding Us to the Obvious.pdf}
}


