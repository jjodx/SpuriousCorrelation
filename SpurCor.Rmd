---
title: Simulating correlations for understanding significance testing, its link to
  estimation of the effect size and how irregularities in the data can yield spurious
  results
author: "Jean-Jacques Orban de Xivry"
date: "April 28, 2019"
output:
  html_document:
    fig_caption: yes
  pdf_document:
    fig_caption: yes
  word_document:
    fig_caption: yes
bibliography: StatLibrary.bib

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval.after = "fig.cap")
library(Hmisc) # for rcorr
library(MASS) # for mvnorm
library(ggplot2)
library(WRS2) # for wincor and pbcor
library(captioner)
library(knitr) # kable function to make tables
fig_nums <- captioner()
```

## To do for JJ

1. Make terminology clearer: population-variable-sample-observations
8. add comments to code
9. make assignments for each section
12. does Bayes Factor solves it? (solve the influence of outliers and subgroups on significance of correlation)


Refer to Bishop blog:

http://deevybee.blogspot.com/2017/11/how-analysis-of-variance-works.html

https://deevybee.blogspot.com/2017/11/anova-t-tests-and-regression-different.html

https://deevybee.blogspot.com/2017/12/using-simulations-to-understand.html

http://deevybee.blogspot.com/2017/12/using-simulations-to-understand-p-values.html

https://www.slideshare.net/deevybishop/introduction-to-simulating-data-to-improve-your-research

Vickers:what is a p-value anyway?
Alex Reinhart: statistics done wrong


# Introduction

Statistical significance is at the heart of a philosophical debate [@wasserstein2016]. Should we abandon the use of p-values [@szucs2017], should we use a more severe significance threshold [@benjamin2018], an adapted one [@lakens2018] or remove it entirely [@amrhein2018a]? These questions are often hotly debated in scientific papers or on social media. Yet, the problem with p-values is that people do not understand them properly or do not understand the circumstances under which a p-value can be used [@colquhoun2014; @gagnier2017a; @greenland2016]. Furthermore, many people do not have a lot of insights how p-values behave in different circumstances. Here, we use our ability to generate random samples of different populations as a tool to get insights into p-values [@colquhoun2014; @martins2018; @ohara2019; @tintle2015]. These simulations of randomly correlated or uncorrelated variables are done with correlations as the statistical test of interest. These simulations highlight and many problems that have been discussed in science. This part adresses the significance problem linked to p-values.

But p-values on their own have very little value if they are not coupled to the effect size of an effect [@hubbard2008]. Indeed, while we sommetimes want to know whether a relationship exist between two variables (e.g. air pollution and respiratory disease), we often also want to know the importance of this link. Does hair pollution explains 0.1 or 25% of respiratory disease incidence. The ability to estimate the importance of such relationship is referred to as the estimation problem. Estimation is important not for distinguishing between two theories but to assess the magnitude of the relationship [@sullivan2012]. The estimation problem is made complicated by publication bias [@joober2012; @mlinaric2017; @song2013]. That is, people do not cherish not significant effect and rarely published them while significant effects are published very easily. We will thus use simulation to understand how publication bias is a threat for the estimation of the magnitude of the effect size.

This publication bias is also linked to the presentation of results that are too good to be true [@francis2013a]. Simulation of artificial data can also provide us with information about the type of results that should be expected [@lakens2017a]. That is, if two variables are correlated and we run three times the same experiment, how many significant p-values do you expect? As EJ Wagenmakers asks (https://twitter.com/EJWagenmakers/status/889987997046910978):
You read two papers, each reporting three preregistered analyses. Which effect is more likely to replicate?
A: p=0.024, p=0.034 and p=0.031
B: p=0.25, p=0.032 and p=0.0001
Note that tackling this question will tell what the expected patterns of p-values are in the presence of an effect. 

The simulations and corresponding p-values reported in this paper were based on the correlation between two variables.n Correlations were chosen because they are very frequently used in scientific papers. The correlation coefficient represents the strength of the association between two variables [@leerodgers1988; @taylor1990]. While the correlation coefficient has been deemed to be robust against violations of the assumptions [@havlicek1976], we now know that it is at least extremely sensitive to outliers [@abdullah1990; @pernet2013]. That is, as we will see below, a slight irregularity in the data is sufficient to obtain significant but spurious correlations.

Therefore, the goal of this paper is to show that simulation of artificial data can be a valid tool to understand the behavior of p-values [@ohara2019; @tintle2015], to get insights into the estimation problem and to find the best technique to account for irregularities in the data.


# Learning about statistics from simulated correlations.

```{r ParamDef, echo=FALSE}
PopSize=15
Nsim=100 # should be 10000
Rval=0
Rval50=0.5
```

Let's imagine the following experiment. Researchers from Klow in Syldavia are willing to measure height and working memory capacity in a population of young healthy participants. Because of their limited budget, they are able to measure these parameters in a population of `r PopSize`  participants. For each individual participant $i$, they have two observations: $(x_i,y_i)$ where $x_i$ is the height of the participant and $y_i$ is his/her working memory capacity. Then, the researchers decided to test the correlation between these two variables for their population.
There is no reason whatsoever to expect a correlation between height and working memory capacity. This should be zero. 
We can simulate this experiment numerically by generating `r PopSize` random numbers with mean of 0 and standard deviation of 1 corresponding to the simulated height of each participant and `r PopSize` different random numbers with mean of 0 and standard deviation of 1 corresponding to the simulated working memory capacity of each participant. A mean of 0 and a standard deviation of 1 were chosen for simplicity but all the arguments below hold if ones picks another mean and/or another standard deviation. By doing this, we have generated `r PopSize` pairs of height and working memory capacity. To follow the idea of our Syldavian researchers, we can then compute the correlation between height and working memory capacity of our randomly generated population.

## simulating a single correlation between two randomly generated samples

In summary, to generate two independent variables (with zero correlations), we will apply the following algorithm:
1. Pick randomly a number from a Gaussian distribution (mean = 0 and standard deviation =1) and assign this number to the height of this participant ($x_i$)
2. Pick randomly a number from a Gaussian distribution (mean = 0 and standard deviation =1) and assign this number to the working memory capacity of this participant ($y_i$)
3. Repeat for every individual $i$ in the population (N= `r PopSize`)
```{r CorrelR0, echo=FALSE}
library(Hmisc) # for rcorr
library(MASS) # for mvnorm
set.seed(12)# to make sure everybody gets the same results
x=mvrnorm(PopSize,0,1,empirical = FALSE)
y=mvrnorm(PopSize,0,1,empirical = FALSE)
XY = data.frame(Xi = x,yi = y)
colnames(XY)<- c("height (xi)","working memory capacity (yi)")
rownames(XY)<- c("Subj # 1","Subj # 2","Subj # 3","Subj # 4","Subj # 5","Subj # 6","Subj # 7","Subj # 8","Subj # 9","Subj # 10","Subj # 11","Subj # 12","Subj # 13","Subj # 14","Subj # 15")
R<-rcorr(x,y)
Scatter1 <- data.frame(x,y)
PP4 <- ggplot(Scatter1, aes(x=x, y=y)) +
  geom_point()+
  geom_smooth(method='lm',se=FALSE)+
  labs(title = "",x="height", y="working memory capacity")
fig_nums("PP4", "Absence of correlation from two randomly generated samples from two uncorrelated variables",display=FALSE)

```
We then obtain the following matrices:
```{r DisplayMatrixR0, echo = FALSE, results = 'asis'}
kable(XY, caption = "randomly generated value for the individuals of the population")
```

These values are unitless. Some of them are positive other negative values.Now that we have generated data for height and working memory capacity for each individual, we can test whether the correlation between these two parameters. We wil first use the Pearson's correlation, which is used in most scientific studies.
Off course, one should not expect any correlation between height and working memory capacity as these are sampled from random independent distributions. The two randomly generated parameters have a correlation coefficient R= `r format(R$r[1,2], digits = 2)` (p-value = `r format(R$P[1,2], digits = 2)`). Interstingly, while the two variables should be independent (height and working memory were generated randomly and independently), their correlation coefficient is not exactly zero. Yet, it is low (and non-significant), which means that we cannot conclude from this data that the variables height and working memory capacity are related (and we cannot conclude that these are not correlated either).
````{r plotR0, echo=FALSE, fig.cap= fig_nums("PP4")} 
PP4
```

Now, this is the data from a single city. Let's know simulate what would happen if the same correlation was computed for `r Nsim` different towns with more than 1000 inhabitants. In this case, we should repeat the process `r Nsim` times in order to know whether the result obtained for `r fig_nums("PP4", display = "cite")` is a special case or not. That is, for every simulated city (N=`r Nsim`), we will generate random numbers for the height and for the working memory capacity. We will then obtain the correlation coefficient and p-value for these parameters for every one of the `r Nsim` towns.

## simulating multiple correlation between two randomly generated samples

Being able to perform these simulations in any programming language has certainly a large added value as we will show below. Therefore, we encourage the interested reader to use the code below to perform those simulations in R. For the readers who do not have the time or the willingness to do so, they can use the web application developed for this paper (See ShinyApp section below).

### doing that in R

Simmulating randomly generated samples can be done in $R$ with the $mvrnorm$ function from the $MASS$ package. To draw `r PopSize` samples from a normal distribution, we can do
``` {r eval=FALSE}
library(Hmisc) # for rcorr
library(MASS) # for mvnorm
set.seed(12)# to make sure everybody gets the same results
# number of individuals in the randomly generated sample
PopSize = 15
# 15 randomly generated number representing the height of the 15 individuals in the population
x=mvrnorm(PopSize,0,1,empirical = FALSE)
# 15 randomly generated number representing the working memory capacity of the 15 individuals in the population
y=mvrnorm(PopSize,0,1,empirical = FALSE)
# correlation between the x and y variables
R<-rcorr(x,y)
# display correlation coefficient
R$r[1,2]
# display corresponding p-values
R$P[1,2]
```
We will then repeat this process `r Nsim` times to obtain a reliable distribution of the p-values. To do so, we will use the ability of the mvnorm function to simultaneously generate random values for the x and y uncorrelated variables. This can be done on the basis of the covariance matrix Sigma:
$$
Sigma = \begin{pmatrix}
1 & 0\\
0 & 1
\end{pmatrix}
$$
where the off-diagonal element of the covariance matrix indicate the theoretical value of the correlation between the variables x and y (Here, R=`r Rval`). Using this covariance matrix, we will use the $mvnorm$ function to build two uncorrelated variables at the same time. This is repeated `r Nsim`  times (Nsim= `r Nsim`) in the code below:

``` {r eval=FALSE}
library(Hmisc) # for rcorr
library(MASS) # for mvnorm
set.seed(123)# to make sure everybody gets the same results
Nsim=1000 # number of simulations
PopSize = 15 #number os elements in each sample
Sigma <- matrix(c(1,0,0,1),2,2)# covariance matrix
res <- data.frame(p=rep(NA,Nsim),R=rep(NA,Nsim),Sig=rep(NA,Nsim)) # pre-allocating space for the dataframe
for (i in c(1:Nsim)){
  # drawing 15 elements for each random variable (x and y) with a covariance matrix equal to Sigma
  # the first column of D corresponds to the samples of the x variable and the second to the samples of the y variable
  D <- mvrnorm(n = PopSize, rep(0, 2), Sigma, empirical = FALSE) 
  R<-rcorr(D[,1],D[,2]) # correlation between the samples of x and y 
  res[i,"R"] <- R$r[1,2] # storing the correlation value for future use
  res[i,"p"]<-R$P[1,2]# storing the p-value for future use
  res[i,"Sig"]<-res[i,"p"]<0.05 # tag p-values smaller than the type II error threshold (here 0.05)
}
```

In the code above the R-dataframe $res$ stores the results of the correlation between x (first colunm of D - $D[,1]$) and y (second column of D = $D[,2]$) for each iteration of the simulations. The value of the correlation coefficient ($R \$ r[1,2]$) and the associated p-value ($R \$ p[1,2]$) are stored in this dataframe. One can use this dataframe to generate the histogram illustrating the distribution of p-values (see `r fig_nums("PP2", display = "cite")` below).

``` {r eval=FALSE}
library(ggplot2) 
# plotting the distribution of p-values
Fcolor <- c("lightblue","orange")
ggplot(res, aes(x=p, fill=Sig)) + 
  geom_histogram(aes(x=p,fill=Sig),binwidth=.05,boundary = 0.05,color="black")+
  scale_fill_manual(values = Fcolor)
```

### using the ShinyApp

For the readers that are not (yet) familiar with the R language, one can simply follow the following steps to be able to interactively follow this tutorial:

1. download and install R and Rstudio (both needed) (e.g. https://courses.edx.org/courses/UTAustinX/UT.7.01x/3T2014/56c5437b88fa43cf828bff5371c6a924/)

2. install the shiny package (run the following from Rstudio console: `install.packages("shiny")`)
3. call the Shiny library (run the following from Rstudio console: `library(shiny)`)
4. run the following from an R console: runGitHub("SpuriousCorrelation","jjodx")

and you can reproduce all the manipulations explained below thanks to the program that is now open (ADD PRINTSCREEN OF PROGRAM).

## What is the expected p-value distribution if there are no correlations

``` {r SimulationR0, echo=FALSE}
set.seed(123)# to make sure everybody gets the same results
Rval = 0 # value of the simulated correlation
res <- data.frame(p=rep(NA,Nsim),R=rep(NA,Nsim),Sig=rep(NA,Nsim))
Sigma <- matrix(c(1,Rval,Rval,1),2,2)
for (i in c(1:Nsim)){
      D <- mvrnorm(n = PopSize, rep(0, 2), Sigma, empirical = FALSE)
      R<-rcorr(D[,1],D[,2])
      res[i,"R"] <- R$r[1,2]
      res[i,"p"]<-R$P[1,2]
      res[i,"Sig"]<-res[i,"p"]<0.05
}

Fcolor <- c("orange","lightblue")
CountSig <- sum(res$Sig)
Psig <- CountSig/length(res$Sig)
res$Sig = factor(1-res$Sig)
PP2 <- ggplot(res, aes(x=p, fill=Sig)) + 
    geom_histogram(aes(x=p,fill=Sig),binwidth=.05,boundary = 0.05,color="black") +
    scale_fill_manual(values = Fcolor) +
    geom_hline(yintercept=CountSig, linetype="dashed", color = "black") +
    geom_label(aes(x = 0, y = CountSig, label = "H0"), 
               hjust = "left", 
               vjust = "bottom", 
               colour = "black", 
               label.size = NA, 
               size = 3,
               fill = NA)+
    coord_cartesian(xlim = c(0, 1))+
    theme(legend.position='none')+
    scale_y_continuous(expand = c(0.1,0)) +
    labs(title="",x="p-value" , y="Count")
fig_nums("PP2", "Distribution of p-values when no effect is present. Each simulated p-value corresponds to the correlation between samples taken from two uncorrelated variables",display=FALSE)
```

Simulating such correlation repeatedly can allow one to look at the distribution of p-values when no correlation is present in the underlying variables. As described in many other papers (Casella,G. and Berger,R. (1990) Statistical Inference. Wadsworth &
Brooks/Cole, Pacific Grove, CA.), this distribution if completely flat and 5% of the p-values are below the typical 5% significance threshold. 

```{r PlotDistribPvalUnderR0, echo=FALSE, fig.cap=fig_nums("PP2")} 
PP2
```

Interestingly, even in the absence of correlation between x and y, there is a subset of correlations associated with a p-value under the significance threshold (type II error). Those are highlighted in orange in `r fig_nums("PP2", display = "cite")` and are called false-positives. That is, these correlations are significant despite the fact that the variables x and y (from which only a few samples was taken) are, by design, not correlated. Given that the distribution of p-values is uniform and that p-values fall in the interval [0,1], there will always be 5% of the p-values under the threshold of 0.05, independently of the number of samples for the two variables.

assignment:
check that increasing the sample size (value of PopSize in the code) does not influence the distribution of p-values when the two variables are unccorelated.

``` {r RectangleErrorTypeR0, echo=FALSE}
d=data.frame(x1=c(0.33,0.33), x2=c(0.66,0.66), y1=c(0,Psig), y2=c(Psig,1), t=c('a','b'))
  PP3_0 <- ggplot() +  
    geom_rect(data=d, mapping=aes(xmin=x1, xmax=x2, ymin=y1, ymax=y2, fill=t), 
              color="black") +
    scale_fill_manual(values = Fcolor, 
                      name="",
                      labels=c("p<=0.05", "p>0.05")) +
    geom_hline(yintercept=0.05, linetype="dashed", color = "red") +
    geom_segment(aes(x = 0.68, y = 0, xend = 0.68, yend = Psig),
     lineend = 'butt', linejoin = 'bevel',
     size = 0.5, arrow = arrow(length = unit(0.2, "inches")),colour="red")+
    geom_label(aes(x = 0.8, y = Psig/2, label = "False-Positive"), 
               hjust = "center", 
               vjust = "bottom", 
               colour = "red", 
               label.size = NA, 
               size = 3,
               fill = NA)+
    geom_label(aes(x = 0.1, y = 0.05, label = "Type 1 \nError threshold"), 
               hjust = "center", 
               vjust = "bottom", 
               colour = "red", 
               label.size = NA, 
               size = 3,
               fill = NA)+
    geom_hline(yintercept=0, color = "black") +
    coord_cartesian(xlim = c(0, 1),ylim = c(0, 1))+
    labs(title="",y="\n \n \nProportion of simulations")+
    scale_y_continuous(expand = c(0,0)) +
    theme(axis.text.x=element_blank(),axis.ticks.x=element_blank())
  fig_nums("PP3_0", paste("proportion of significant and non-significant p-values when the simulated R is ", Rval, "and the sample size is", PopSize),display=FALSE)
```

```{r PlotRectangleR0, echo=FALSE, fig.cap=fig_nums("PP3_0")} 
PP3_0
```
In the absence of correlation between the underlying variables, the significant p-values are referred to as false-positive. We expect 5% of such correlations for a type I error threshold (aka the infamous significance threshold) of 0.05 (red horizontal dashed line in `r fig_nums("PP3_0", display = "cite")`).
### understanding the influence of sample size (and power) on the magnitude of the false-positive correlations

``` {r DistriRunderR0, echo=FALSE}
  Rsig <- mean(res$R[res$Sig==0])
  Rall <- mean(res$R)
  if (Rsig>Rval){Rpos <- c("left","right")}else{Rpos <- c("right","left")}
PP1 <- ggplot(res, aes(x=R, fill=Sig)) + 
    geom_histogram(aes(x=R,fill=Sig),binwidth=.05,boundary = 0,color="black") +
    scale_fill_manual(values = Fcolor) +
    coord_cartesian(xlim = c(-1, 1))+
    geom_vline(xintercept=Rsig, linetype="dashed", color = "darkorange", size = 1) +
    geom_label(aes(x = Rsig, y = Inf, label = "Sig."), 
               hjust = Rpos[1], 
               vjust = "top", 
               colour = "darkorange", 
               label.size = NA, 
               size = 3,
               fill = NA)+
    geom_vline(xintercept=Rall, linetype="dashed", color = "darkblue", size = 1) +
    geom_label(aes(x = Rall, y=Inf, label = "All"),
               hjust = Rpos[2], 
               vjust = "top", 
               colour = "darkblue", 
               label.size = NA, 
               size = 3,
               fill = NA)+
    theme(legend.position='none') +
    scale_y_continuous(expand = c(0.15,0)) +
    labs(title="",x="correlation coefficient" , y="Count")
fig_nums("PP1", paste("distribution of the correlations coefficient when R=", Rval, "and N=", PopSize),display=FALSE)
```
The distribution of the correlation coefficient will allow us to understand the problem with small sample size and why significant results always look convincing with small sample size. In `r fig_nums("PP1", display = "cite")`, one can see that the simulated correlations with `r PopSize` samples span a range between `r format(min(res$R),digits=2)` and `r format(max(res$R),digits=2)`. However, only larger positive correlations (in orange on `r fig_nums("PP1", display = "cite")`) are significant. That is, with small sample size (here `r PopSize` observations for each variable), the false-positive correlations are large (absolute value > `r format(min(abs(res$R[res$Sig==0])), digits=2)`).

```{r PlotDistriRunderR0, echo=FALSE, fig.cap=fig_nums("PP1")} 
PP1
```

This if often referred to as the significance fallalcy (REF). If it so big and significant, it can only be true. "Such a large correlation can only be true" would say an inexperienced scientist. It is therefore important to note that, when correlating two variables and looking at a high and significant correlation, it is impossible to say whether this is a true effect or a false-positive corelation. Replication of this correlation in an independent and hopefully larger sample can only increase the probability that this correlation is true if it is again significant. In the other case, it increases the probability that the first correlation was a false-positive one. Yet, even two correlations are insufficient to draw a firm conclusion about the veracity of the effect but they at least give an better idea of the investigated correlation.

In other words high-correlations on a limited number of datapoints should require skepticism for any scientists. I am well aware that it is not always easy to do so (see the beautiful correlation on N=6 in Orban de Xivry et al. 2009).

``` {r DistriRN75, echo=FALSE}
PopSize75 = 75 #number os elements in each sample
res75 <- data.frame(p=rep(NA,Nsim),R=rep(NA,Nsim),Sig=rep(NA,Nsim)) # pre-allocating space for the dataframe
for (i in c(1:Nsim)){
  # drawing 75 elements for each random variable (x and y) with a covariance matrix equal to Sigma
  D <- mvrnorm(n = PopSize75, rep(0, 2), Sigma, empirical = FALSE) 
  R<-rcorr(D[,1],D[,2]) # correlation between the x and y variables
  res75[i,"R"] <- R$r[1,2] # storing the correlation value for further use
  res75[i,"p"]<-R$P[1,2]# storing the p-value for further use
  res75[i,"Sig"]<-res75[i,"p"]<0.05 # tag p-values smaller than the type II error threshold (here 0.05)
}
CountSig75 <- sum(res75$Sig)
Psig75 <- CountSig75/length(res75$Sig)
res75$Sig = factor(1-res75$Sig)
Rsig <- mean(res75$R[res75$Sig==0])
Rall <- mean(res75$R)
if (Rsig>Rall){Rpos <- c("left","right")}else{Rpos <- c("right","left")}
PP175 <- ggplot(res75, aes(x=R, fill=Sig)) + 
  geom_histogram(aes(x=R,fill=Sig),binwidth=.05,boundary = 0,color="black") +
  scale_fill_manual(values = Fcolor) +
  coord_cartesian(xlim = c(-1, 1))+
  geom_vline(xintercept=Rsig, linetype="dashed", color = "darkorange", size = 1) +
  geom_label(aes(x = Rsig, y = Inf, label = "Sig."), 
             hjust = Rpos[1], 
             vjust = "top", 
             colour = "darkorange", 
             label.size = NA, 
             size = 3,
             fill = NA)+
  geom_vline(xintercept=Rall, linetype="dashed", color = "darkblue", size = 1) +
  geom_label(aes(x = Rall, y=Inf, label = "All"),
             hjust = Rpos[2], 
             vjust = "top", 
             colour = "darkblue", 
             label.size = NA, 
             size = 3,
             fill = NA)+
  theme(legend.position='none') +
  scale_y_continuous(expand = c(0.15,0)) +
  labs(title="",x="correlation coefficient" , y="Count")
fig_nums("PP175", paste("distribution of the correlations coefficient when R=", Rval, "and the sample size is", PopSize75),display=FALSE)
```

I have insisted on the fact that the high false-positive correlations described above were linked to the small sample size used (N=`r PopSize`). Now, we will investigate the influence of sample size on the magnitude of these high false-positive correlations. To do so, we will repeat the simulations above but with a sample size of N=`r PopSize75` instead of N=`r PopSize`. As one can see on `r fig_nums("PP175", display = "cite")`, the value of the false-positive correlations has been dramatically reduced to absolute values larger than `r format(min(abs(res75$R[res75$Sig==0])), digits=2)`). In other words, it is easys to obtain high false positive correlations with low sample size and much more difficult to do so with high sample size.

``` {r PlotDistriRN75, echo=FALSE, fig.cap=fig_nums("PP175")}
PP175
```



The significance fallacy has important consequences for the publishability of findings. If two researchers are (unknowingly) reporting a false-positive correlation with N=`r PopSize` and R=0.7 for researcher A and with N=`r PopSize75` and R=0.4 for researcher B, which one will more easily publish in a high-impact journal (whatever that means...)? Therefore, any scientist should be warry on exploratory correlations with low N.

### understanding the multiple comparison problem.

``` {r MultComp, echo=FALSE}
Index <- c(1:Nsim)
SigIndex <- Index[res$Sig==TRUE]
DifSig <- SigIndex - c(1,SigIndex[1:length(SigIndex)-1])+1


```

These simulations also show that if we keep simulating random variables, we are assured of getting at least one significant correlation. With `r Nsim` simulations, there is 100% chance that at least one correlation will be significant, even if there is no correlation between the x and y variables. In our simulation, we observed one significant correlation after `r format(mean(DifSig),digits=0)` trials. That is, if one keeps testing the significance of correlation between different variables, one is assured of finding a significant correlation even if none actually exists. This is often referred to as the multiple comparison problem and has been famously used by scientists to show brain activity in a dead salmon.

This illustrates that if someone keeps trying to test correlation between many possible variables, the probability of getting a significant correlation even if there should be no correlation.

```{r SetR50, echo=FALSE}
Rval50 = 0.5
```

## simulating a single correlation R=`r Rval50`

It is also interesting to simulate cases where the correlation is not zero. While simulating correlations with R=0 thaught us about false-positive correlations (type I error), the importance of sample size and the multiple comparison problem, simulating corelation with $R\ne0$ can teach us about power (and type II error), the precision of estimation and the consequences of publishing only significant studies.

To simulate two variables with a given correlation, one can  use the $mvnorm$ function but this time with a covariance matrix exhibiting the value for the simulated correlation on the off-diagonal elements. For instance, if we want to simulate two variables with a correlation of R=`r Rval50`, then the covariance matrix is:
$$
Sigma = \begin{pmatrix}
1 & `r Rval50`\\
`r Rval50` & 1
\end{pmatrix}
$$


```{r CorrelR50, echo=FALSE}
set.seed(12)# to make sure everybody gets the same results
Sigma <- matrix(c(1,Rval50,Rval50,1),2,2)
D <- mvrnorm(n = PopSize, rep(0, 2), Sigma, empirical = FALSE) 
  R<-rcorr(D[,1],D[,2]) # correlation between the x and y variables
Scatter2 <- data.frame(x=D[,1],y=D[,2])
PP450 <- ggplot(Scatter2, aes(x=x, y=y)) +
  geom_point()+
  geom_smooth(method='lm',formula=y~x)+
  labs(title = "",x="Random variable 1", y="Random variable 2")
fig_nums("PP450", paste("example of correlations when the simulated R is ", Rval50, "and the sample size is", PopSize),display=FALSE)

XY = data.frame(Xi = D[,1],yi = D[,2])
colnames(XY)<- c("height (xi)","weight")
rownames(XY)<- c("Subj # 1","Subj # 2","Subj # 3","Subj # 4","Subj # 5","Subj # 6","Subj # 7","Subj # 8","Subj # 9","Subj # 10","Subj # 11","Subj # 12","Subj # 13","Subj # 14","Subj # 15")
```



```{r DisplayMatrixR50, echo = FALSE, results = 'asis'}
kable(XY, caption = "randomly generated value for the individuals of the population")
```

The generated values can be seen in the table below. One can see that individuals that appear to have a larger height also have a larger weight. This is even more apparent on  `r fig_nums("PP450", display = "cite")` where the relationship between height and weight is visible.

```{r plotCorrelR50, echo=FALSE, fig.cap=fig_nums("PP450")}
PP450
```
Together, these two variables are correlated with a correlation coefficient R= `r format(R$r[1,2], digits = 3)` (p-value = `r format(R$P[1,2], digits=3)`). That is, the correlation between the two variables is not exactly equal to the simulated value (R= `r Rval50`) We will see later how we can recover the actual simulated correlation.This is reminiscent of when we generated uncorrelated samples. Even though there was no correlation in the underlying variables (height and working memory capacity), the correlations that we found were not exactly zero but around zero.

## What is the expected p-value distribution if there is a correlation

Now, we will repeat this process a number of times in order to obtain the distribution of the p-values when there is a correlation R=`r Rval50`as we did earlier when no correlation was present. The distribution of p-values was then completely flat (see `r fig_nums("PP2", display = "cite")`). In contrast, When there is an underlying correlation between the two variables, the distribution of p-values becomes skewed with more p-values under 0.05.
``` {r SimulationR50, echo=FALSE}
set.seed(123)# to make sure everybody gets the same results
res <- data.frame(p=rep(NA,Nsim),R=rep(NA,Nsim),Sig=rep(NA,Nsim))
Sigma <- matrix(c(1,Rval50,Rval50,1),2,2)
for (i in c(1:Nsim)){
      D <- mvrnorm(n = PopSize, rep(0, 2), Sigma, empirical = FALSE)
      R<-rcorr(D[,1],D[,2])
      res[i,"R"] <- R$r[1,2]
      res[i,"p"]<-R$P[1,2]
      res[i,"Sig"]<-res[i,"p"]<0.05
}

Fcolor <- c("orange","lightblue")
CountSig <- sum(res$Sig)
Psig <- CountSig/length(res$Sig)
res$Sig = factor(1-res$Sig)
PP2_50 <- ggplot(res, aes(x=p, fill=Sig)) + 
    geom_histogram(aes(x=p,fill=Sig),binwidth=.05,boundary = 0.05,color="black") +
    scale_fill_manual(values = Fcolor) +
    coord_cartesian(xlim = c(0, 1))+
    theme(legend.position='none')+
    scale_y_continuous(expand = c(0.1,0)) +
    labs(title="",x="p-value" , y="Count")
fig_nums("PP2_50", paste("distribution of p-values when the simulated R is ", Rval50, "and the sample size is", PopSize),display=FALSE)
```

``` {r PlotDistriP_R50, echo=FALSE, fig.cap=fig_nums("PP2_50")}
PP2_50
```

`r fig_nums("PP2_50",display="cite")` also shows that non-significant p-values cannot be considered as a proof that there are no correlation between the variables x and y. All the non-significant correlations on `r fig_nums("PP2_50",display="cite")` are false-negative. That is, these correlations are are not significant even though the sampled variables x and y are correlated. False-negative correlations correspond to type II error, which is linked to the power of an experiment.

``` {r RectangleErrorTypeR50, echo=FALSE}
d=data.frame(x1=c(0.33,0.33), x2=c(0.66,0.66), y1=c(0,Psig), y2=c(Psig,1), t=c('a','b'))
  PP3_50 <- ggplot() +  
    geom_rect(data=d, mapping=aes(xmin=x1, xmax=x2, ymin=y1, ymax=y2, fill=t), 
              color="black") +
    scale_fill_manual(values = Fcolor, 
                      name="",
                      labels=c("p<=0.05", "p>0.05")) +
    geom_segment(aes(x = 0.68, y = 1, xend = 0.68, yend = Psig),
     lineend = 'butt', linejoin = 'bevel',
     size = 0.5, arrow = arrow(length = unit(0.2, "inches")))+
    geom_label(aes(x = 0.8, y = Psig+((1-Psig)/2), label = "False-Negative"), 
               hjust = "center", 
               vjust = "bottom", 
               colour = "black", 
               label.size = NA, 
               size = 3,
               fill = NA)+
    geom_hline(yintercept=0.8, linetype="dashed", color = "black") +
    geom_label(aes(x = 0.1, y = 0.8, label = "Type 2 \nError threshold"), 
               hjust = "center", 
               vjust = "center", 
               colour = "black", 
               label.size = NA, 
               size = 3,
               fill = NA)+
    geom_hline(yintercept=0, color = "black") +
    coord_cartesian(xlim = c(0, 1),ylim = c(0, 1))+
    labs(title="",y="\n \n \nProportion of simulations")+
    scale_y_continuous(expand = c(0,0)) +
    theme(axis.text.x=element_blank(),axis.ticks.x=element_blank())
  fig_nums("PP3_50", paste("proportion of significant and non-significant p-values when the simulated R is ", Rval50, "and the sample size is", PopSize),display=FALSE)
```

### understanding power

The percentage of significant correlations is equal the power of an experiment designed to detect a correlation of R=`r Rval50` with `r PopSize` observations. Indeed, the power of an experiment can be defined as the chance that an experiment designed to detect a given effect (here a correlation) will actually do so (100% minus power is also referred to as type II error or false negative). In our case, we performed as many experiments as we simulated correlations but only `r format(100*Psig, digits = 1)`% from them were significant. That means that if we have `r PopSize` observations from two populations that are correlated with R=`r Rval50`, we have `r format(100*Psig, digits = 0)`% of detecting a significant correlation (see `r fig_nums("PP3_50", display="cite")` for an illustration).

``` {r PlotRectangleErrorTypeR50, echo=FALSE, fig.cap=fig_nums("PP3_50")}
PP3_50
```

``` {r SimulationPvalue4power_N28, echo=FALSE}
PopSize28 = 28
set.seed(123)# to make sure everybody gets the same results
Nsim=1000
  res28 <- data.frame(p=rep(NA,Nsim),R=rep(NA,Nsim),Sig=rep(NA,Nsim))
Sigma <- matrix(c(1,Rval50,Rval50,1),2,2)
for (i in c(1:Nsim)){
      D <- mvrnorm(n = PopSize28, rep(0, 2), Sigma, empirical = FALSE)
      R<-rcorr(D[,1],D[,2])
      res28[i,"R"] <- R$r[1,2]
      res28[i,"p"]<-R$P[1,2]
      res28[i,"Sig"]<-res28[i,"p"]<0.05
}

Fcolor <- c("orange","lightblue")
CountSig28 <- sum(res28$Sig)
Psig28 <- CountSig28/length(res28$Sig)
res28$Sig = factor(1-res28$Sig)
d=data.frame(x1=c(0.33,0.33), x2=c(0.66,0.66), y1=c(0,Psig28), y2=c(Psig28,1), t=c('a','b'))
  PP28 <- ggplot() +  
    geom_rect(data=d, mapping=aes(xmin=x1, xmax=x2, ymin=y1, ymax=y2, fill=t), 
              color="black") +
    scale_fill_manual(values = Fcolor, 
                      name="",
                      labels=c("p<=0.05", "p>0.05")) +
    geom_segment(aes(x = 0.68, y = 1, xend = 0.68, yend = Psig28),
     lineend = 'butt', linejoin = 'bevel',
     size = 0.5, arrow = arrow(length = unit(0.2, "inches")))+
    geom_label(aes(x = 0.8, y = Psig28+((1-Psig28)/2), label = "False-Negative"), 
               hjust = "center", 
               vjust = "bottom", 
               colour = "black", 
               label.size = NA, 
               size = 3,
               fill = NA)+
    geom_hline(yintercept=0.8, linetype="dashed", color = "black") +
    geom_label(aes(x = 0.1, y = 0.8, label = "Type 2 \nError threshold"), 
               hjust = "center", 
               vjust = "center", 
               colour = "black", 
               label.size = NA, 
               size = 3,
               fill = NA)+
    geom_hline(yintercept=0, color = "black") +
    coord_cartesian(xlim = c(0, 1),ylim = c(0, 1))+
    labs(title="",y="\n \n \nProportion of simulations")+
    scale_y_continuous(expand = c(0,0)) +
    theme(axis.text.x=element_blank(),
          axis.ticks.x=element_blank())
  fig_nums("PP28", paste("proportion of significant and non-significant p-values when the simulated R is ", Rval50, "and the sample size is", PopSize28),display=FALSE)
```

As indicated on `r fig_nums("PP28",display="cite")`, the expected power of an experiment is 80% (or 90%). Yet, this is here not the case. To detect a correlation R=`r Rval`, one needs `r PopSize28` observations. In this case, the percentage of significant correlations reaches the power requirement of 80%. The power of an experiment depends on the size of the effect (here, the strength of the correlation) and on the number of samples. 
``` {r RectangleErrorTypeR50_N28, echo=FALSE, fig.cap=fig_nums("PP28")}
PP28
```

This manipulations demonstrate how simulations can yield insights about the required sample size for detecting a correlation of a given value (Here, R=`r Rval50`). The problem with power is that we rarely now in advance how big the effect size (e.g. the strengthh the correlation) will be.

Assignment: test the influence of effect size on power

### effect of sample size on the accuracy of correlation estimation.

``` {r DistriR_R50_N15, echo=FALSE}
  Rsig <- mean(res$R[res$Sig==0])
  Rall <- mean(res$R)
  if (Rsig>Rval50){Rpos <- c("left","right")}else{Rpos <- c("right","left")}
PP150 <- ggplot(res, aes(x=R, fill=Sig)) + 
    geom_histogram(aes(x=R,fill=Sig),binwidth=.05,boundary = 0,color="black") +
    scale_fill_manual(values = Fcolor) +
    coord_cartesian(xlim = c(-1, 1))+
    geom_vline(xintercept=Rsig, linetype="dashed", color = "darkorange", size = 1) +
    geom_label(aes(x = Rsig, y = Inf, label = "Sig."), 
               hjust = Rpos[1], 
               vjust = "top", 
               colour = "darkorange", 
               label.size = NA, 
               size = 3,
               fill = NA)+
    geom_vline(xintercept=Rall, linetype="dashed", color = "darkblue", size = 1) +
    geom_label(aes(x = Rall, y=Inf, label = "All"),
               hjust = Rpos[2], 
               vjust = "top", 
               colour = "darkblue", 
               label.size = NA, 
               size = 3,
               fill = NA)+
    theme(legend.position='none') +
    scale_y_continuous(expand = c(0.15,0)) +
    labs(title="",x="correlation coefficient" , y="Count")
  fig_nums("PP150", paste("distribution of significant and non-significant correlation coefficients when the simulated R is ", Rval50, "and the sample size is", PopSize),display=FALSE)
```

``` {r PlotDistriR_R50_N15, echo=FALSE, fig.cap=fig_nums("PP150")}
PP150
```

``` {r DistriR_R50_N28, echo=FALSE}
  Rsig28 <- mean(res28$R[res28$Sig==0])
  Rall28 <- mean(res28$R)
  if (Rsig28>Rall28){Rpos <- c("left","right")}else{Rpos <- c("right","left")}
PP150_28 <- ggplot(res28, aes(x=R, fill=Sig)) + 
    geom_histogram(aes(x=R,fill=Sig),binwidth=.05,boundary = 0,color="black") +
    scale_fill_manual(values = Fcolor) +
    coord_cartesian(xlim = c(-1, 1))+
    geom_vline(xintercept=Rsig28, linetype="dashed", color = "darkorange", size = 1) +
    geom_label(aes(x = Rsig28, y = Inf, label = "Sig."), 
               hjust = Rpos[1], 
               vjust = "top", 
               colour = "darkorange", 
               label.size = NA, 
               size = 3,
               fill = NA)+
    geom_vline(xintercept=Rall28, linetype="dashed", color = "darkblue", size = 1) +
    geom_label(aes(x = Rall28, y=Inf, label = "All"),
               hjust = Rpos[2], 
               vjust = "top", 
               colour = "darkblue", 
               label.size = NA, 
               size = 3,
               fill = NA)+
    theme(legend.position='none') +
    scale_y_continuous(expand = c(0.15,0)) +
    labs(title="",x="correlation coefficient" , y="Count")
fig_nums("PP150_28", paste("distribution of significant and non-significant correlation coefficients when the simulated R is ", Rval50, "and the sample size is", PopSize28),display=FALSE)
```

On `r fig_nums("PP150_28",display="cite")`, we can see that while many simulated correlations are around the value of `r Rval50`, the value of the obtained correlations are spread over a large interval. Indeed, the simulated correlations exhibited a large 95% confidence interval: between `r format(quantile(res$R,0.025),digits = 2)` and  `r format(quantile(res$R,0.975),digits = 2)`. One can look at the effect of sample size on the precision of the estimated correlation. It is obvious that increasing the sample size (N= `r PopSize28`) decreases the 95% confidence interval of simulated correlation ([`r format(quantile(res28$R,0.025),digits = 2)`,`r format(quantile(res28$R,0.975),digits = 2)`]). This has been described previously and illustrates that high power (here 80%) does not always correspond to high precision given the remaining variability of the simulated correlations.

``` {r PlotDistriR_R50_N28, echo=FALSE, fig.cap=fig_nums("PP150_28")}
PP150_28
```

### understanding that publication bias inflates effect size

`r fig_nums("PP150_28",display="cite")` also indicates the importance of publishing non-significant findings. Indeed, if one looks at the average correlation for the significant correlations (orange dashed vertical line in `r fig_nums("PP150_28",display="cite")`, with N= `r PopSize28`), this average correlation (mean R=`r format(Rsig28,digits=3)`) overestimates the actual correlation value (R= `r Rval50`). In contrast, the average correlation over all samples (darkblue dashed vertical line on the graphs above) is much closer to the actual simulated correlation (with N= `r PopSize28`, average correlation is `r format(Rall28,digits=3)`). Furhermore, this overestimation of the actual correlation is even bigger with smaller sample sizes (with N= `r PopSize`, average correlation from significant ones is R=`r format(Rsig,digits=3)` while average correlation from all studies is R= `r format(Rall,digits=3)`)
If we look at the mean correlation from only significant studies, it is always larger than the actual population average correlation.

### How can results be too good to be true?

``` {r 2Good2Btrue, echo=FALSE}
set.seed(123)# to make sure everybody gets the same results
Ngood <- 5
Good2B5 <- data.frame(NSig=rep(NA,Nsim))
Sigma <- matrix(c(1,Rval50,Rval50,1),2,2)
for (i in c(1:Nsim)){
   tmp <- data.frame(p=rep(NA,Ngood),Sig=rep(NA,Ngood))
  for (j in c(1:Ngood)){
      D <- mvrnorm(n = PopSize, rep(0, 2), Sigma, empirical = FALSE)
      R<-rcorr(D[,1],D[,2])
      tmp[j,"p"]<-R$P[1,2]
      tmp[j,"Sig"]<-tmp[j,"p"]<0.05
  }
  Good2B5[i,"NSig"]<-sum(tmp$Sig)/Ngood
}
Yall<- sum(Good2B5$NSig>0.99)
Nall = 100*Yall/Nsim

Plot2B <- ggplot(Good2B5, aes(x=NSig)) + 
    geom_label(aes(x = 1, y=Yall, label = paste(Nall, "%")),
               hjust = "right", 
               vjust = "bottom", 
               colour = "red", 
               label.size = NA,
               size = 3,
               fill = NA)+
    geom_histogram(binwidth=.05,boundary = 0.05,color="black") +
    coord_cartesian(xlim = c(0, 1))+
    theme(legend.position='none')+
    scale_y_continuous(expand = c(0.1,0)) +
    labs(title="",x="percentage of significant simulations" , y="Count")
fig_nums("Plot2B", paste("percentage of significant simulations among", Ngood ,"simulations for R =", Rval50, "and sample size =", PopSize),display=FALSE)
```

It is clear from `r fig_nums("PP150",display="cite")` and `r fig_nums("PP150_28",display="cite")` that, even in the presence of an effect, some correlations will turn out to be non-significant. Then, it is interesting to look at the probability of getting one or two non-significant results in a series of `r Ngood` experiments. To do so, we simulated such series of `r Ngood` experiments `r Nsim` times and the counted the proportion of significant correlation.

To get a sense of the outcomes of this stimulation, one can use the Shiny App and repeatedly simulate `r Ngood` experiments (setting number of simulations to `r Ngood`, actual correlation to `r Rval50` and Population size to `r PopSize`). In most of the cases, the `r Ngood` experiments do not yield `r Ngood` significant p-values. Actually, this only happens in `r Nall`% of the simulations. 

``` {r Plot2Good2Btrue, echo=FALSE, fig.cap=fig_nums("Plot2B")}
Plot2B
```

``` {r 2Good2Btrue28, echo=FALSE}
set.seed(123)# to make sure everybody gets the same results
Good2B528 <- data.frame(NSig=rep(NA,Nsim))
for (i in c(1:Nsim)){
   tmp <- data.frame(p=rep(NA,Ngood),Sig=rep(NA,Ngood))
  for (j in c(1:Ngood)){
      D <- mvrnorm(n = PopSize28, rep(0, 2), Sigma, empirical = FALSE)
      R<-rcorr(D[,1],D[,2])
      tmp[j,"p"]<-R$P[1,2]
      tmp[j,"Sig"]<-tmp[j,"p"]<0.05
  }
  Good2B528[i,"NSig"]<-sum(tmp$Sig)/Ngood
}
Yall28 <- sum(Good2B528$NSig>0.99)
Nall28 = 100*Yall28/Nsim
```

This result is really important because it means that scientists who are only presenting series of papers with only significant studies (often with small sample size, hence low power) are not telling you the entire truth. Either they have plenty of studies in their file-drawer or they are use questionable practice to always obtain significant p-values. I personally find it depressing to assist to presentations with series of only significant studies with small sample size.

Note that the percentage of series of `r Ngood` simulations with `r Ngood` significant studies increases with power and sample size (N=`r PopSize`: `r Nall`%; N=`r PopSize28`: `r Nall28`%). Yet, even with 80% power, these remain the minority (`r 100-Nall28`% of the series of `r Ngood` simulations contain at least one non-significant p-value).

``` {r 2Good2Btrue28_10, echo=FALSE}
set.seed(123)# to make sure everybody gets the same results
Ngood_10<-10
Good2B528_10 <- data.frame(NSig=rep(NA,Nsim))
for (i in c(1:Nsim)){
   tmp <- data.frame(p=rep(NA,Ngood_10),Sig=rep(NA,Ngood_10))
  for (j in c(1:Ngood_10)){
      D <- mvrnorm(n = PopSize28, rep(0, 2), Sigma, empirical = FALSE)
      R<-rcorr(D[,1],D[,2])
      tmp[j,"p"]<-R$P[1,2]
      tmp[j,"Sig"]<-tmp[j,"p"]<0.05
  }
  Good2B528_10[i,"NSig"]<-sum(tmp$Sig)/Ngood_10
}
Yall28_10 <- sum(Good2B528_10$NSig>0.99)
Nall28_10 = 100*Yall28_10/Nsim
```

In contrast, the probability of observing only significant p-values decreases with the number of experiments considered in one series. For instance, when considering series of `r Ngood_10` experiments, the probability of observing `r Ngood_10` significant p-values is only `r Nall28_10`%, despite having 80% power. Again, this reinforces the conclusion that every scientists should now and then report experiments that failed. Yet, we know this is rarely the case and these scientists present results that are too good to be true.

## extending this line of thought to the T-test

if we follow the same line of reasoning, one can investigate the distribution of p-values obtained from t-test aiming at comparing two independent groups. That is, instead of correlating the variables obtained from the random sampling, one can simply compare them with a t-test (the covariance matrix should have zeros on his off-diagonal elements). To simulate a difference between the two groups, one needs to change the mean of one of the groups. 

# Learning about sensitivity of correlations from simulated correlations.
The use of correlation is widespread in science. Yet, correlations are very sensitive to violations of the assumptions. Here, we will show that 1) in the absence of correlation (R=0), the presence of an outlier dramatically increases the false-positive rate (more than 5% of correlations are significant when R=0); 2) if data are pooled between two different groups (e.g. young adults vs. old adults) and the means for the two groups differ on the two variables.

## the outlier problem: simulation and solution

``` {r ExampleOutlier, echo=FALSE}
set.seed(123)# to make sure everybody gets the same results
Sigma <- matrix(c(1,Rval,Rval,1),2,2)
SDdist <- 3
POP <- c("Main","outlier")
D <- mvrnorm(n = PopSize, rep(0, 2), Sigma, empirical = FALSE)
mMeans<-apply(D,2,mean)
mStd<-apply(D,2,sd)
D[PopSize,]<-mMeans+SDdist*mStd
Z <- c(rep(0,PopSize-1),1)
Scatter1 <- data.frame(D1=D[,1],D2=D[,2],Zf= factor(Z))
Rout<-rcorr(D[,1],D[,2])

cols <- c("royalblue3", "orangered2" )
Outlier4 <- ggplot(Scatter1) + 
  geom_point(aes(x=D1, y=D2, color=Zf))+
  geom_smooth(aes(x=D1, y=D2),method='lm',se=FALSE,formula = y~x)+
  scale_color_manual(name = "Population(s)", 
                     values = cols,
                     labels = POP)+
  labs(title = "",x="Random variable 1" , y="Random variable 2")
fig_nums("Outlier4", paste("example of a correlation biased by an outlier (in red) when the simulated R is ", Rval, ", the sample size is", PopSize, " and the outlier is", SDdist, "standard deviation away from the mean"),display=FALSE)
```

To generate an outlier, we will first generate the x and y parameteres for the 'r PopSize' individuals. We will then compute their mean and standard deviation, the data from the last individual will then be replaced by a point 'r SDdist' standard deviation away from the mean (mean + 3*SD). This will be done for both x and y variables for this individual. By doing so, one obtains the following grahics.

``` {r PlotExampleOutlier, echo=FALSE, fig.cap=fig_nums("Outlier4")}
Outlier4
```

In this case, we observe a strong correlation R=`r format(Rout$r[1,2], digits = 2)` (p=`r format(Rout$P[1,2], digits = 2)`). However, it is unclear whether more correlations will turn out to be significant in the presence of an outlier. Therefore, as we did above, we will generate many such samples and test whether the presence of an outlier increases the chance of getting a significant correlation.

### Simulation: How sensisitive are correlations to one outlier?

``` {r SimulationOutlier, echo=FALSE}
set.seed(123)# to make sure everybody gets the same results
resOut <- data.frame(p=rep(NA,Nsim),R=rep(NA,Nsim),Sig=rep(NA,Nsim))
Sigma <- matrix(c(1,Rval,Rval,1),2,2)
for (i in c(1:Nsim)){
      D <- mvrnorm(n = PopSize, rep(0, 2), Sigma, empirical = FALSE)
      mMeans<-apply(D,2,mean)
      mStd<-apply(D,2,sd)
      D[PopSize,]<-mMeans+SDdist*mStd
      Z <- c(rep(0,PopSize-1),1)
      R<-rcorr(D[,1],D[,2])
      resOut[i,"R"] <- R$r[1,2]
      resOut[i,"p"]<-R$P[1,2]
      resOut[i,"Sig"]<-resOut[i,"p"]<0.05
}

Fcolor <- c("orange","lightblue")
CountSigOut <- sum(resOut$Sig)
PsigOut <- CountSigOut/length(resOut$Sig)
resOut$Sig = factor(1-resOut$Sig)
PP2Out <- ggplot(resOut, aes(x=p, fill=Sig)) + 
    geom_histogram(aes(x=p,fill=Sig),binwidth=.05,boundary = 0.05,color="black") +
    scale_fill_manual(values = Fcolor) +
    geom_hline(yintercept=CountSigOut, linetype="dashed", color = "black") +
    geom_label(aes(x = 0, y = CountSigOut, label = "H0"), 
               hjust = "left", 
               vjust = "bottom", 
               colour = "black", 
               label.size = NA, 
               size = 3,
               fill = NA)+
    coord_cartesian(xlim = c(0, 1))+
    theme(legend.position='none')+
    scale_y_continuous(expand = c(0.1,0)) +
    labs(title="",x="p-value" , y="Count")
fig_nums("PP2Out", paste("distribution of p-values when the simulated correlations are biased by an outlier ", SDdist," standard deviation away from the mean. True R= ", Rval, "and the sample size is", PopSize),display=FALSE)

d=data.frame(x1=c(0.33,0.33), x2=c(0.66,0.66), y1=c(0,PsigOut), y2=c(PsigOut,1), t=c('a','b'))
  PP3Out <- ggplot() +  
    geom_rect(data=d, mapping=aes(xmin=x1, xmax=x2, ymin=y1, ymax=y2, fill=t), 
              color="black") +
    scale_fill_manual(values = Fcolor, 
                      name="",
                      labels=c("p<=0.05", "p>0.05")) +
    geom_hline(yintercept=0.05, linetype="dashed", color = "red") +
    geom_label(aes(x = 0.1, y = 0.05, label = "Type 1 \nError"), 
               hjust = "center", 
               vjust = "bottom", 
               colour = "red", 
               label.size = NA, 
               size = 3,
               fill = NA)+
    geom_segment(aes(x = 0.68, y = 0, xend = 0.68, yend = PsigOut),
     lineend = 'butt', linejoin = 'bevel',
     size = 0.5, arrow = arrow(length = unit(0.2, "inches")),colour="red")+
    geom_label(aes(x = 0.8, y = PsigOut/2, label = "False-Positive"), 
               hjust = "center", 
               vjust = "bottom", 
               colour = "red", 
               label.size = NA, 
               size = 3,
               fill = NA)+
    
    geom_hline(yintercept=0.8, linetype="dashed", color = "black") +
    geom_label(aes(x = 0.1, y = 0.8, label = "Type 2 \nError"), 
               hjust = "center", 
               vjust = "center", 
               colour = "black", 
               label.size = NA, 
               size = 3,
               fill = NA)+
    geom_hline(yintercept=0, color = "black") +
    coord_cartesian(xlim = c(0, 1),ylim = c(0, 1))+
    labs(title="",y="\n \n \nProportion of simulations")+
    scale_y_continuous(expand = c(0,0)) +
    theme(axis.text.x=element_blank(),
          axis.ticks.x=element_blank())
  fig_nums("PP3Out", paste("proportion of significant and non-significant p-values when the simulated correlations are biased by an outlier ", SDdist," standard deviation away from the mean. True R= ", Rval, "and the sample size is", PopSize),display=FALSE)
```

We will again repeatedly generate random numbers to obtain samples of the x and y variables. In addition, we replace the last point with an outlier as we did above. Importantly, given that these two variables are randomly generated, the expected correlation is R=0. Therefore, we expect a uniform distribution of p-values. In addition, only 5% of the simulated correlations should be significant as we have also detailed above (Fig???). The two graphs below illustrates that both of these expectations are falsified by the data. Therefore, a single outlier in a population of `r PopSize` points increase the type I error tremendously. That is, there are `r format(PsigOut*100, digits=0)` % significant p-values!

``` {r PlotDistriP_outlier, echo=FALSE, fig.cap=fig_nums("PP2Out")}
PP2Out
```

``` {r PlotProportionP_outlier, echo=FALSE, fig.cap=fig_nums("PP3Out")}
PP3Out
```

The interesting reader can use the Shiny App or the R code to explore the influnce of population size and the distance of the oultier from the mean on the increase in  type I error caused by the outlier. Off course, this is also reflected in the value of the average correlation coefficient.

Assignment: These analyses have been performed on the basis of R=0. The intersted reader can explore the influence of such outlier on simulated correlations with R>0.

## testing solutions

How do we know that the solution is good? If type I error rate is 5% and if average correlation is closed to simulated one.

### Solution: non-parametric correlation - Spearman
``` {r SimulationSpearOutlier, echo=FALSE}
set.seed(123)# to make sure everybody gets the same results
resSpear <- data.frame(p=rep(NA,Nsim),R=rep(NA,Nsim),Sig=rep(NA,Nsim))
Sigma <- matrix(c(1,Rval,Rval,1),2,2)
for (i in c(1:Nsim)){
      D <- mvrnorm(n = PopSize, rep(0, 2), Sigma, empirical = FALSE)
      mMeans<-apply(D,2,mean)
      mStd<-apply(D,2,sd)
      D[PopSize,]<-mMeans+SDdist*mStd
      Z <- c(rep(0,PopSize-1),1)
      R<-rcorr(D[,1],D[,2],type = 'spearman')
      resSpear[i,"R"] <- R$r[1,2]
      resSpear[i,"p"]<-R$P[1,2]
      resSpear[i,"Sig"]<-resSpear[i,"p"]<0.05
}

Fcolor <- c("orange","lightblue")
CountSigSpear <- sum(resSpear$Sig)
PsigSpear <- CountSigSpear/length(resSpear$Sig)
resSpear$Sig = factor(1-resSpear$Sig)
d=data.frame(x1=c(0.33,0.33), x2=c(0.66,0.66), y1=c(0,PsigSpear), y2=c(PsigSpear,1), t=c('a','b'))
  PP3Spear <- ggplot() +  
    geom_rect(data=d, mapping=aes(xmin=x1, xmax=x2, ymin=y1, ymax=y2, fill=t), 
              color="black") +
    scale_fill_manual(values = Fcolor, 
                      name="",
                      labels=c("p<=0.05", "p>0.05")) +
    geom_hline(yintercept=0.05, linetype="dashed", color = "red") +
    geom_label(aes(x = 0.1, y = 0.05, label = "Type 1 \nError"), 
               hjust = "center", 
               vjust = "bottom", 
               colour = "red", 
               label.size = NA, 
               size = 3,
               fill = NA)+
    geom_segment(aes(x = 0.68, y = 0, xend = 0.68, yend = PsigSpear),
     lineend = 'butt', linejoin = 'bevel',
     size = 0.5, arrow = arrow(length = unit(0.2, "inches")),colour="red")+
    geom_label(aes(x = 0.8, y = PsigSpear/2, label = "False-Positive"), 
               hjust = "center", 
               vjust = "bottom", 
               colour = "red", 
               label.size = NA, 
               size = 3,
               fill = NA)+
    geom_hline(yintercept=0.8, linetype="dashed", color = "black") +
    geom_label(aes(x = 0.1, y = 0.8, label = "Type 2 \nError"), 
               hjust = "center", 
               vjust = "center", 
               colour = "black", 
               label.size = NA, 
               size = 3,
               fill = NA)+
    geom_hline(yintercept=0, color = "black") +
    coord_cartesian(xlim = c(0, 1),ylim = c(0, 1))+
    labs(title="",y="\n \n \nProportion of simulations")+
    scale_y_continuous(expand = c(0,0)) +
    theme(axis.text.x=element_blank(),
          axis.ticks.x=element_blank())
  fig_nums("PP3Spear",paste("Ability of Spearman correlation to reduce the influence of an outlier on type I error. Proportion of significant and non-significant p-values for Spearman correlation when the simulated correlations are biased by an outlier ", SDdist," standard deviation away from the mean. True R= ", Rval, "and the sample size is", PopSize),display = FALSE)
```

A first solution is to use a non-parametric rank correlation such as Spearman (DEFINE). Here, the graph below shows that the influence of the outlier is much reduced and the type I error rate (= `r format(PsigSpear*100,digits=0)`%) is not very close to the 5% level.
``` {r PlotSpearOultier, echo=FALSE, , fig.cap=fig_nums("PP3Spear")}
PP3Spear
```

### Solution: robust parametric correlation

``` {r SimulationWincorOutlier, echo=FALSE}
set.seed(123)# to make sure everybody gets the same results
resRob <- data.frame(p=rep(NA,Nsim),R=rep(NA,Nsim),Sig=rep(NA,Nsim))
Sigma <- matrix(c(1,Rval,Rval,1),2,2)
for (i in c(1:Nsim)){
      D <- mvrnorm(n = PopSize, rep(0, 2), Sigma, empirical = FALSE)
      mMeans<-apply(D,2,mean)
      mStd<-apply(D,2,sd)
      D[PopSize,]<-mMeans+SDdist*mStd
      Z <- c(rep(0,PopSize-1),1)
      R<-wincor(D[,1],D[,2],0.2)
      resRob[i,"R"] <- R$cor
      resRob[i,"p"]<-R$p.value
      resRob[i,"Sig"]<-resRob[i,"p"]<0.05
}

Fcolor <- c("orange","lightblue")
CountSigRob <- sum(resRob$Sig)
PsigRob <- CountSigRob/length(resRob$Sig)
resRob$Sig = factor(1-resRob$Sig)
d=data.frame(x1=c(0.33,0.33), x2=c(0.66,0.66), y1=c(0,PsigRob), y2=c(PsigRob,1), t=c('a','b'))
  PP3Rob <- ggplot() +  
    geom_rect(data=d, mapping=aes(xmin=x1, xmax=x2, ymin=y1, ymax=y2, fill=t), 
              color="black") +
    scale_fill_manual(values = Fcolor, 
                      name="",
                      labels=c("p<=0.05", "p>0.05")) +
    geom_hline(yintercept=0.05, linetype="dashed", color = "red") +
    geom_label(aes(x = 0.1, y = 0.05, label = "Type 1 \nError threshold"), 
               hjust = "center", 
               vjust = "bottom", 
               colour = "red", 
               label.size = NA, 
               size = 3,
               fill = NA)+
    geom_segment(aes(x = 0.68, y = 0, xend = 0.68, yend = PsigRob),
     lineend = 'butt', linejoin = 'bevel',
     size = 0.5, arrow = arrow(length = unit(0.2, "inches")),colour="red")+
    geom_label(aes(x = 0.8, y = PsigRob/2, label = "False-Positive"), 
               hjust = "center", 
               vjust = "bottom", 
               colour = "red", 
               label.size = NA, 
               size = 3,
               fill = NA)+
    geom_hline(yintercept=0.8, linetype="dashed", color = "black") +
    geom_label(aes(x = 0.1, y = 0.8, label = "Type 2 \nError threshold"), 
               hjust = "center", 
               vjust = "center", 
               colour = "black", 
               label.size = NA, 
               size = 3,
               fill = NA)+
    geom_hline(yintercept=0, color = "black") +
    coord_cartesian(xlim = c(0, 1),ylim = c(0, 1))+
    labs(title="",y="\n \n \nProportion of simulations")+
    scale_y_continuous(expand = c(0,0)) +
    theme(axis.text.x=element_blank(),
          axis.ticks.x=element_blank())
    fig_nums("PP3Rob",paste("Ability of robust correlation (winsorized) to reduce the influence of an outlier on type I error. Proportion of significant and non-significant p-values for robust correlation when the simulated correlations are biased by an outlier ", SDdist," standard deviation away from the mean. True R= ", Rval, "and the sample size is", PopSize),display = FALSE)
```

In contrast, the use of winsorized correlation (DEFINE) solves the problem as robust correlation methods are designed to handle outliers. This is confirmed by the graph below, which shows that the influence of the outlier is eliminated and the type I error rate (= `r PsigRob`) is very close to the 5% level.

``` {r PlotRobOutlier, echo=FALSE, fig.cap=fig_nums("PP3Rob")}
PP3Rob
```

Mention package where robust corelaions can be found in R

## influence of subgroups on correlation: simulation and solution

Researchers want to estimate correlation across different subgroups.For instance, in my own research, I wanted to correlate the amount of explicit motor learning with working memory capacity across both young and old participants. Yet, working memory capacity is known to decline with aging and so is the explicit component of motor adaptation. This does not mean that these two variables should be correlated across participants. The goal of this section is to investigate the influence of subgroups on correlations. We will start by illustrate the problem via simulations.

### How sensisitive are correlations to subgroups?

``` {r SimulationSubgroup, echo=FALSE}
set.seed(123)# to make sure everybody gets the same results
Sigma <- matrix(c(1,Rval,Rval,1),2,2)
SDdistSub <- 2
POP <- c("SubGroup1","SubGroup2")
D <- mvrnorm(n = PopSize, rep(0, 2), Sigma, empirical = FALSE)
SubN = round(PopSize/2)
D[1:SubN,]<-D[1:SubN,]+matrix(rep(SDdistSub,len=SubN*2), nrow = SubN)
Z <- c(rep(0,SubN),rep(1,PopSize-SubN))
D1 <- D[,1]
D2 <- D[,2]
Scatter1 <- data.frame(D1=D[,1],D2=D[,2],Zf= factor(Z))
Rsub<-rcorr(D[,1],D[,2])

cols <- c("royalblue3", "orangered2" )
SubG4 <- ggplot(Scatter1) + 
  geom_point(aes(x=D1, y=D2, color=Zf))+
  geom_smooth(aes(x=D1, y=D2),method='lm',se=FALSE,formula = y~x)+
  scale_color_manual(name = "Population(s)", 
                     values = cols,
                     labels = POP)+
  labs(title = "",x="Random variable 1" , y="Random variable 2")
  fig_nums("SubG4",paste("Example of correlation biased by differences between subgroups", SDdistSub," standard deviation away from each other. True R= ", Rval, "and the sample size is", PopSize),display = FALSE)
```

To generate subgroups, we first generate the x and y parameteres for the 'r PopSize' individuals. We will then split the population into two subgroups and add a value of `r SDdistSub` to each the values of both varialbes for one of the two subgroups. By doing so, one obtains the following grahics.

``` {r PlotExampleSubgroup, echo=FALSE, fig.cap=fig_nums("SubG4")}
SubG4
```

``` {r SimulationSubgroups, echo=FALSE}
set.seed(12)# to make sure everybody gets the same results
resSub <- data.frame(p=rep(NA,Nsim),R=rep(NA,Nsim),Sig=rep(NA,Nsim),NoDifButCorrel=rep(NA,Nsim))
resSub75 <- data.frame(p=rep(NA,Nsim),R=rep(NA,Nsim),Sig=rep(NA,Nsim),NoDifButCorrel=rep(NA,Nsim))
resSub75_15 <- data.frame(p=rep(NA,Nsim),R=rep(NA,Nsim),Sig=rep(NA,Nsim),NoDifButCorrel=rep(NA,Nsim))
Sigma <- matrix(c(1,Rval,Rval,1),2,2)
SDdistSub75 = 0.5;
for (i in c(1:Nsim)){
  D <- mvrnorm(n = PopSize, rep(0, 2), Sigma, empirical = FALSE)
  SubN = round(PopSize/2)
  D[1:SubN,]<-D[1:SubN,]+matrix(rep(SDdistSub,len=SubN*2), nrow = SubN)
  Z <- c(rep(0,SubN),rep(1,PopSize-SubN))
  R<-rcorr(D[,1],D[,2])
  resSub[i,"R"] <- R$r[1,2]
  resSub[i,"p"]<-R$P[1,2]
  resSub[i,"Sig"]<-resSub[i,"p"]<0.05
  T1 <- t.test(D[,1]~Z) # where y is numeric and x is a binary factor 
  T2 <- t.test(D[,2]~Z) # where y is numeric and x is a binary factor
  resSub[i,"NoDifButCorrel"] <- (((T2$p.value>0.05) + (T1$p.value>0.05)) * (resSub[i,"p"]<0.05))>0
  
  ## SDdist of 0.75 and N=PopSize
  D <- mvrnorm(n = PopSize, rep(0, 2), Sigma, empirical = FALSE)
  SubN = round(PopSize/2)
  D[1:SubN,]<-D[1:SubN,]+matrix(rep(SDdistSub75,len=SubN*2), nrow = SubN)
  Z <- c(rep(0,SubN),rep(1,PopSize-SubN))
  R<-rcorr(D[,1],D[,2])
  resSub75_15[i,"R"] <- R$r[1,2]
  resSub75_15[i,"p"]<-R$P[1,2]
  resSub75_15[i,"Sig"]<-resSub75_15[i,"p"]<0.05
  
  ## SDdist of 0.75 and N=PopSize75
  D <- mvrnorm(n = PopSize75, rep(0, 2), Sigma, empirical = FALSE)
  SubN = round(PopSize75/2)
  D[1:SubN,]<-D[1:SubN,]+matrix(rep(SDdistSub75,len=SubN*2), nrow = SubN)
  Z <- c(rep(0,SubN),rep(1,PopSize75-SubN))
  R<-rcorr(D[,1],D[,2])
  resSub75[i,"R"] <- R$r[1,2]
  resSub75[i,"p"]<-R$P[1,2]
  resSub75[i,"Sig"]<-resSub75[i,"p"]<0.05
}

Fcolor <- c("orange","lightblue")
#general case
CountSigSub <- sum(resSub$Sig)
PsigSub <- CountSigSub/length(resSub$Sig)
## SDdist of 0.75 and N=PopSize
CountSigSub75_15 <- sum(resSub75_15$Sig)
PsigSub75_15 <- CountSigSub75_15/length(resSub75_15$Sig)
## SDdist of 0.75 and N=PopSize75
CountSigSub75 <- sum(resSub75$Sig)
PsigSub75 <- CountSigSub75/length(resSub75$Sig)

CountSigSubNDBC <- sum(resSub$NoDifButCorrel)
PsigNDBC <- CountSigSubNDBC/length(resSub$NoDifButCorrel)

resSub$Sig = factor(1-resSub$Sig)
PP2Sub <- ggplot(resSub, aes(x=p, fill=Sig)) + 
    geom_histogram(aes(x=p,fill=Sig),binwidth=.05,boundary = 0.05,color="black") +
    scale_fill_manual(values = Fcolor) +
    geom_hline(yintercept=CountSigSub, linetype="dashed", color = "black") +
    geom_label(aes(x = 0, y = CountSigSub, label = "H0"), 
               hjust = "left", 
               vjust = "bottom", 
               colour = "black", 
               label.size = NA, 
               size = 3,
               fill = NA)+
    coord_cartesian(xlim = c(0, 1))+
    theme(legend.position='none')+
    scale_y_continuous(expand = c(0.1,0)) +
    labs(title="",x="p-value" , y="Count")
  fig_nums("PP2Sub",paste("distribution of p-values when the simulated correlations are biased by subgroup differences. Here, the subgroups are ", SDdist," standard deviation away from each other. True R= ", Rval, "and the sample size is", PopSize),display = FALSE)

d=data.frame(x1=c(0.33,0.33), x2=c(0.66,0.66), y1=c(0,PsigSub), y2=c(PsigSub,1), t=c('a','b'))
  PP3Sub <- ggplot() +  
    geom_rect(data=d, mapping=aes(xmin=x1, xmax=x2, ymin=y1, ymax=y2, fill=t), 
              color="black") +
    scale_fill_manual(values = Fcolor, 
                      name="",
                      labels=c("p<=0.05", "p>0.05")) +
    geom_hline(yintercept=0.05, linetype="dashed", color = "red") +
    geom_segment(aes(x = 0.68, y = 0, xend = 0.68, yend = PsigSub),
     lineend = 'butt', linejoin = 'bevel',
     size = 0.5, arrow = arrow(length = unit(0.2, "inches")),colour="red")+
    geom_label(aes(x = 0.8, y = PsigSub/2, label = "False-Positive"), 
               hjust = "center", 
               vjust = "bottom", 
               colour = "red", 
               label.size = NA, 
               size = 3,
               fill = NA)+
    geom_label(aes(x = 0.1, y = 0.05, label = "Type 1 \nError threshold"), 
               hjust = "center", 
               vjust = "bottom", 
               colour = "red", 
               label.size = NA, 
               size = 3,
               fill = NA)+
    geom_hline(yintercept=0.8, linetype="dashed", color = "black") +
    geom_label(aes(x = 0.1, y = 0.8, label = "Type 2 \nError threshold"), 
               hjust = "center", 
               vjust = "center", 
               colour = "black", 
               label.size = NA, 
               size = 3,
               fill = NA)+
    geom_hline(yintercept=0, color = "black") +
    coord_cartesian(xlim = c(0, 1),ylim = c(0, 1))+
    labs(title="",y="\n \n \nProportion of simulations")+
    scale_y_continuous(expand = c(0,0)) +
    theme(axis.text.x=element_blank(),
          axis.ticks.x=element_blank())
  
    fig_nums("PP3Sub",paste("Influence of subgroup differences on type I error. Proportion of significant and non-significant p-values when the simulated correlations are biased by subgroups ", SDdistSub," standard deviation away from each other. True R= ", Rval, "and the sample size is", PopSize),display = FALSE)
```
We observe a strong correlation R=`r format(Rsub$r[1,2], digits = 2)` (p=`r format(Rsub$P[1,2], digits = 2)`). Repeating such simulations will allow us to understand the effect of subgroups on the number of false-positive correlations.The two graphs below illustrates that the presence of differences between the subgroups have a massive effect on the simulated correlations. For instance, if the two subgroups are 1SD away, we observe `r format(PsigSub*100, digits=0)` % significant p-values! which is far above the expected 5% threshold. 

``` {r PlotDistriP_subgroups, echo=FALSE,fig.cap=fig_nums("PP2Sub")}
PP2Sub
```

``` {r PlotProportionP_subgroups, echo=FALSE,fig.cap=fig_nums("PP3Sub")}  
PP3Sub
```

Note that for `r format(PsigNDBC*100, digit=0)`% of these significant correlations, the between groups difference was not significant. Yet, these subgroups yielded a positive correlation.

In addition, the bigger the groups get, the smaller the difference between the subgroups must be to give rise to a significant correlation. For instance, for a sample of `r PopSize`, a difference of `r SDdistSub75` standard deviation between group does not inflate the type I error rate (`r format(PsigSub75_15*100, digits=0)`% of correlations are significant). However, this difference becomes important for larger populations. For instance, a difference of `r SDdistSub75` increase the type I error rate to `r PsigSub75` and this number increases with sample size.

it is self-evident that these subgroups will also affect the magnitude of the false-positive correlations, which will tend to be larger because of the subgroups. This can easily be visualized with the ShinyApp. 

### Solution to the subgroup problem

We will first show that Spearman correlation is again unable to solve the problem of subgroups eventhough it slightly reduces the problem. Then we will show that one needs to take 

### Spearmann is not a solution

``` {r SimulationSpear_subgroups, echo=FALSE}
set.seed(12)# to make sure everybody gets the same results
resSubSpear <- data.frame(p=rep(NA,Nsim),R=rep(NA,Nsim),Sig=rep(NA,Nsim),NoDifButCorrel=rep(NA,Nsim))
Sigma <- matrix(c(1,Rval,Rval,1),2,2)
for (i in c(1:Nsim)){
  D <- mvrnorm(n = PopSize, rep(0, 2), Sigma, empirical = FALSE)
  SubN = round(PopSize/2)
  D[1:SubN,]<-D[1:SubN,]+matrix(rep(SDdistSub,len=SubN*2), nrow = SubN)
  Z <- c(rep(0,SubN),rep(1,PopSize-SubN))
  R<-rcorr(D[,1],D[,2],type='spearman')
  resSubSpear[i,"R"] <- R$r[1,2]
  resSubSpear[i,"p"]<-R$P[1,2]
  resSubSpear[i,"Sig"]<-resSubSpear[i,"p"]<0.05
}

Fcolor <- c("orange","lightblue")
#general case
CountSigSubSpear <- sum(resSubSpear$Sig)
PsigSubSpear <- CountSigSubSpear/length(resSubSpear$Sig)

d=data.frame(x1=c(0.33,0.33), x2=c(0.66,0.66), y1=c(0,PsigSubSpear), y2=c(PsigSubSpear,1), t=c('a','b'))
  PP3SubSpear <- ggplot() +  
    geom_rect(data=d, mapping=aes(xmin=x1, xmax=x2, ymin=y1, ymax=y2, fill=t), 
              color="black") +
    scale_fill_manual(values = Fcolor, 
                      name="",
                      labels=c("p<=0.05", "p>0.05")) +
    geom_hline(yintercept=0.05, linetype="dashed", color = "red") +
    geom_segment(aes(x = 0.68, y = 0, xend = 0.68, yend = PsigSubSpear),
     lineend = 'butt', linejoin = 'bevel',
     size = 0.5, arrow = arrow(length = unit(0.2, "inches")),colour="red")+
    geom_label(aes(x = 0.8, y = PsigSubSpear/2, label = "False-Positive"), 
               hjust = "center", 
               vjust = "bottom", 
               colour = "red", 
               label.size = NA, 
               size = 3,
               fill = NA)+
    geom_label(aes(x = 0.1, y = 0.05, label = "Type 1 \nError threshold"), 
               hjust = "center", 
               vjust = "bottom", 
               colour = "red", 
               label.size = NA, 
               size = 3,
               fill = NA)+
    geom_hline(yintercept=0.8, linetype="dashed", color = "black") +
    geom_label(aes(x = 0.1, y = 0.8, label = "Type 2 \nError threshold"), 
               hjust = "center", 
               vjust = "center", 
               colour = "black", 
               label.size = NA, 
               size = 3,
               fill = NA)+
    geom_hline(yintercept=0, color = "black") +
    coord_cartesian(xlim = c(0, 1),ylim = c(0, 1))+
    labs(title="",y="\n \n \nProportion of simulations")+
    scale_y_continuous(expand = c(0,0)) +
    theme(axis.text.x=element_blank(),
          axis.ticks.x=element_blank())
      fig_nums("PP3SubSpear",paste("Inability of Spearman correlation to reduce the influence of subgroups on type I error. Proportion of significant and non-significant p-values for Spearman correlation when the simulated correlations are biased by subgroups ", SDdist," standard deviation away from each other. True R= ", Rval, "and the sample size is", PopSize),display = FALSE)
```

``` {r PlotProportionSpear_subgroups, echo=FALSE,fig.cap=fig_nums("PP3SubSpear")}
  PP3SubSpear
```

### Taking subgroups into account

``` {r SimulationRegression, echo=FALSE}
set.seed(12)# to make sure everybody gets the same results
# Regression
resReg <- data.frame(p=rep(NA,Nsim),R=rep(NA,Nsim),Sig=rep(NA,Nsim),NoDifButCorrel=rep(NA,Nsim))
Sigma <- matrix(c(1,Rval,Rval,1),2,2)
for (i in c(1:Nsim)){
  D <- mvrnorm(n = PopSize, rep(0, 2), Sigma, empirical = FALSE)
  SubN = round(PopSize/2)
  D[1:SubN,]<-D[1:SubN,]+matrix(rep(SDdistSub,len=SubN*2), nrow = SubN)
  Z <- c(rep(0,SubN),rep(1,PopSize-SubN))
  Add <- data.frame(D1=D[,1],D2=D[,2],Z=Z)
  Rrr <- lm(D2 ~ D1+Z, Add)
  SumRrr <- summary(Rrr, correlation = TRUE)
  resReg[i,"R"] <-SumRrr$coefficients[2,1] #Normalized coefficient
  resReg[i,"p"] <- SumRrr$coefficients[2,4] #p-value
  resReg[i,"Sig"]<-resReg[i,"p"]<0.05
}

Fcolor <- c("orange","lightblue")
#general case
CountSigReg <- sum(resReg$Sig)
PsigReg <- CountSigReg/length(resReg$Sig)
resReg$Sig = factor(1-resReg$Sig)
d=data.frame(x1=c(0.33,0.33), x2=c(0.66,0.66), y1=c(0,PsigReg), y2=c(PsigReg,1), t=c('a','b'))
  PP3Reg <- ggplot() +  
    geom_rect(data=d, mapping=aes(xmin=x1, xmax=x2, ymin=y1, ymax=y2, fill=t), 
              color="black") +
    scale_fill_manual(values = Fcolor, 
                      name="",
                      labels=c("p<=0.05", "p>0.05")) +
    geom_hline(yintercept=0.05, linetype="dashed", color = "red") +
    geom_segment(aes(x = 0.68, y = 0, xend = 0.68, yend = PsigReg),
     lineend = 'butt', linejoin = 'bevel',
     size = 0.5, arrow = arrow(length = unit(0.2, "inches")),colour="red")+
    geom_label(aes(x = 0.8, y = PsigReg/2, label = "False-Positive"), 
               hjust = "center", 
               vjust = "bottom", 
               colour = "red", 
               label.size = NA, 
               size = 3,
               fill = NA)+
    geom_label(aes(x = 0.1, y = 0.05, label = "Type 1 \nError threshold"), 
               hjust = "center", 
               vjust = "bottom", 
               colour = "red", 
               label.size = NA, 
               size = 3,
               fill = NA)+
    geom_hline(yintercept=0.8, linetype="dashed", color = "black") +
    geom_label(aes(x = 0.1, y = 0.8, label = "Type 2 \nError threshold"), 
               hjust = "center", 
               vjust = "center", 
               colour = "black", 
               label.size = NA, 
               size = 3,
               fill = NA)+
    geom_hline(yintercept=0, color = "black") +
    coord_cartesian(xlim = c(0, 1),ylim = c(0, 1))+
    labs(title="",y="\n \n \nProportion of simulations")+
    scale_y_continuous(expand = c(0,0)) +
    theme(axis.text.x=element_blank(),
          axis.ticks.x=element_blank())
  
  fig_nums("PP3Reg",paste("Ability of regression to reduce the influence of subgroups on type I error. Proportion of significant and non-significant p-values for standardized regression coefficient when the simulated correlations are biased by subgroups ", SDdist," standard deviation away from each other. True R= ", Rval, "and the sample size is", PopSize),display = FALSE)

  RsigReg <- mean(resReg$R[resReg$Sig==0])
  RallReg <- mean(resReg$R)
  if (RsigReg>Rval){Rpos <- c("left","right")}else{Rpos <- c("right","left")}
PP1Reg <- ggplot(resReg, aes(x=R, fill=Sig)) + 
    geom_histogram(aes(x=R,fill=Sig),binwidth=.05,boundary = 0,color="black") +
    scale_fill_manual(values = Fcolor) +
    coord_cartesian(xlim = c(-1, 1))+
    geom_vline(xintercept=RsigReg, linetype="dashed", color = "darkorange", size = 1) +
    geom_label(aes(x = Rsig, y = Inf, label = "Sig."), 
               hjust = Rpos[1], 
               vjust = "top", 
               colour = "darkorange", 
               label.size = NA, 
               size = 3,
               fill = NA)+
    geom_vline(xintercept=RallReg, linetype="dashed", color = "darkblue", size = 1) +
    geom_label(aes(x = RallReg, y=Inf, label = "All"),
               hjust = Rpos[2], 
               vjust = "top", 
               colour = "darkblue", 
               label.size = NA, 
               size = 3,
               fill = NA)+
    theme(legend.position='none') +
    scale_y_continuous(expand = c(0.15,0)) +
    labs(title="",x="correlation coefficient" , y="Count")
fig_nums("PP1Reg", paste("distribution of significant and non-significant standardized regression coefficients when the simulated correlations are biased by subgroups ", SDdist," standard deviation away from each other. True R is ", Rval, "and the sample size is", PopSize),display=FALSE)
```

When correlating parameters across different subgroups, one has to take them into account. This can be done by means of regression. In a simple linear regression $ y = a*x + b $, the coefficient $a$ corresponds to the correlation coefficient between x and y if these variables are z-normalized. To z-normalize a variable, one needs to substract its mean from every observation and divide the outcome by the standard deviation of the sample ($z_i = (x_i - mean(x))/SD(x)$). Note that when the data are normalized, $b$ is always zero> To take subgroups into account, one needs to modify the regression question as follow:
$$
y = a*x + c*G
$$ 
where $G$ is equal to zero for one subgroup and one for the other. This equation makes the hypothesis that we expect the same correlation for both groups given that there are no interaction term between $x$ and $G$. 

``` {r PlotProportionReg_subgroups, echo=FALSE, fig.cap=fig_nums("PP3Reg")}
PP3Reg
```

Estimation of correlation coefficients in the presence of two subgroups looks different than when there are no subgroups. 

``` {r PlotDistriR0_Reg_subgroups, echo=FALSE, fig.cap=fig_nums("PP1Reg")}
PP1Reg
```

``` {r, echo=FALSE}
set.seed(12)# to make sure everybody gets the same results
# Regression
resReg50 <- data.frame(p=rep(NA,Nsim),R=rep(NA,Nsim),Sig=rep(NA,Nsim),NoDifButCorrel=rep(NA,Nsim))
Sigma <- matrix(c(1,Rval50,Rval50,1),2,2)
for (i in c(1:Nsim)){
  D <- mvrnorm(n = PopSize, rep(0, 2), Sigma, empirical = FALSE)
  SubN = round(PopSize/2)
  D[1:SubN,]<-D[1:SubN,]+matrix(rep(SDdistSub,len=SubN*2), nrow = SubN)
  Z <- c(rep(0,SubN),rep(1,PopSize-SubN))
  Add <- data.frame(D1=D[,1],D2=D[,2],Z=Z)
  Rrr <- lm(D2 ~ D1+Z, Add)
  SumRrr <- summary(Rrr, correlation = TRUE)
  resReg50[i,"R"] <-SumRrr$coefficients[2,1] #Normalized coefficient
  resReg50[i,"p"] <- SumRrr$coefficients[2,4] #p-value
  resReg50[i,"Sig"]<-resReg50[i,"p"]<0.05
}

Fcolor <- c("orange","lightblue")
#general case
CountSigReg50 <- sum(resReg50$Sig)
PsigReg50 <- CountSigReg50/length(resReg50$Sig)
resReg50$Sig = factor(1-resReg50$Sig)

  RsigReg50 <- mean(resReg50$R[resReg50$Sig==0])
  RallReg50 <- mean(resReg50$R)
  if (RsigReg50>Rval50){Rpos <- c("left","right")}else{Rpos <- c("right","left")}
PP1Reg50 <- ggplot(resReg50, aes(x=R, fill=Sig)) + 
    geom_histogram(aes(x=R,fill=Sig),binwidth=.05,boundary = 0,color="black") +
    scale_fill_manual(values = Fcolor) +
    coord_cartesian(xlim = c(-1, 1))+
    geom_vline(xintercept=RsigReg50, linetype="dashed", color = "darkorange", size = 1) +
    geom_label(aes(x = RsigReg50, y = Inf, label = "Sig."), 
               hjust = Rpos[1], 
               vjust = "top", 
               colour = "darkorange", 
               label.size = NA, 
               size = 3,
               fill = NA)+
    geom_vline(xintercept=RallReg50, linetype="dashed", color = "darkblue", size = 1) +
    geom_label(aes(x = RallReg50, y=Inf, label = "All"),
               hjust = Rpos[2], 
               vjust = "top", 
               colour = "darkblue", 
               label.size = NA, 
               size = 3,
               fill = NA)+
    theme(legend.position='none') +
    scale_y_continuous(expand = c(0.15,0)) +
    labs(title="",x="correlation coefficient" , y="Count")
fig_nums("PP1Reg50", paste("distribution of significant and non-significant standardized regression coefficients when the simulated correlations are biased by subgroups ", SDdist," standard deviation away from each other. True R is ", Rval50, "and the sample size is", PopSize),display=FALSE)
```
Taking subgroups into account does not prevent us from detecting correlations when they are present. Indeed, one can apply the same logic when a correlation of `r Rval50` is used for the simulations. In this case, we see that the average correlation over all simulations (R=`r format(RallReg50,digits=2)`) is pretty close to the actual correlation value (R=`r Rval50`). This futher validates the use of this method to compute correlation in the presence of subgroups. Off course, it remains also true that considering only the significant correlations remains problematic and leads to an overestimation of the actual correlation value (orange dashed line on the graph below).

``` {r PlotDistriR50_Reg_subgroups, echo=FALSE, fig.cap=fig_nums("PP1Reg50")}
PP1Reg50
```



## it can become more complicated.

Simulation to demonstrate how you should decide which covariates to adjust for in the context of a randomized controlled trial.
https://twitter.com/statsepi/status/1115902270888128514

# What have we learned?
Simulations can be  useful to get insights on statistics.

1) distribution of p-value under no correlation is flat. Same probability of observing p<0.05 than p>0.95

2) Even in the presence of an effect, some p-values can be non-significant.

3) In the presence of an effect, proportion of significant pvalue provide information about power. In the absence of an effect, it represents the number of false positive.

4) publication bias is bad for effect size estimation

5) when power is low, increase in effect size overestimation, increase in CI for estimated R, increase in the effect of outliers

6) It is impossible to never observe non-signifcant p-values. Some scientists present resuls that are too good to be true.

7) Simulations can be useful to test potential solution to known problems. For instance, I did not know whether Spearman correlation was able to handle outliers or not. The simulations clearly showed that it does not.


# References


